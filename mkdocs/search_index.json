{
    "docs": [
        {
            "location": "/",
            "text": "Welcome to the CalTRACK Technical Documentation\n\n\nA Shared repository for beta testers of CalTRACK methods\n\n\nAdditional documentation on the CalTRACK process can be found a \ncaltrack.org\n.\n\n\n\n\nThe CalTRACK Beta Test is intended to help test, refine and finalize the technical requirements and methods for CalTRACK. In it, the initial draft requirements for CalTRACK developed by the technical working group will be tested in the field using data from PG&E to empirically verify assumptions made by the technical working group, identify areas of sensitivity, and agree on a first implementable system. Details about the Beta Test plan can can be found here.\n\n\n\n\nCommunication for the project will happen primarily on Slack. Any relevant changes to the technical specification outlined in the CalTRACK docs will be discussed and resolved as Github issues on \nthis repository\n\n\nContributors:\n\n\n\n\nOpen Energy Efficiency\n\n\nEnergySavvy\n\n\nDNV GL",
            "title": "Home"
        },
        {
            "location": "/#welcome-to-the-caltrack-technical-documentation",
            "text": "A Shared repository for beta testers of CalTRACK methods  Additional documentation on the CalTRACK process can be found a  caltrack.org .   The CalTRACK Beta Test is intended to help test, refine and finalize the technical requirements and methods for CalTRACK. In it, the initial draft requirements for CalTRACK developed by the technical working group will be tested in the field using data from PG&E to empirically verify assumptions made by the technical working group, identify areas of sensitivity, and agree on a first implementable system. Details about the Beta Test plan can can be found here.   Communication for the project will happen primarily on Slack. Any relevant changes to the technical specification outlined in the CalTRACK docs will be discussed and resolved as Github issues on  this repository  Contributors:   Open Energy Efficiency  EnergySavvy  DNV GL",
            "title": "Welcome to the CalTRACK Technical Documentation"
        },
        {
            "location": "/guides/v1guide/",
            "text": "Guide to Using CalTRACK v1.0 Monthly Gross Savings Methods\n\n\nThe CalTRACK Version 1.0 specification provides guidance for performing monthly billing analysis for whole home weather normalized gross savings for groups of residential energy efficiency projects.\n\n\nIt includes data requirements and technical specifications for data preparation and cleaning, site-level billing analysis, and aggregation of site-level results.\n\n\nThe complete technical specification for v1 monthly methods is found in four documents\n\n\n\n\nmonthly/data-sources.md\n describes the necessary data requirements for running CalTRACK v1\n\n\nmonthly/data-prep.md\n describes the sequence of steps required for preparing data for analysis\n\n\n/monthly/analysis.md\n describes the methods use for calculating site-level savings using monthly data\n\n\nmonthly/aggregation.md\n describes the methods use for aggregating site-level savings to group or portfolio average and total savings\n\n\n\n\nThe specification is intended to be done in order to ensure consistency and replicability.\n\n\nSupported Use Cases\n\n\nThe CalTRACK v1 monthly billing analysis methods were developed to produce a reliable estimate of basic, weather normalized gross savings at the site level and weighted average gross savings for groups (or portfolios) of projects.\n\n\nThis definition of gross savings was developed with two specific use cases in mind, and was not intended to extend to all potential uses of monthly billing analysis.\n\n\nThe two use cases that CalTRACK v1 is intended to support are:\n\n\n\n\nCalculating \npayable savings\n in the PG&E Residential Third-Party Pay-for-Performance pilot program. In this program, selected third party aggregators (organization providing services that may lead to energy savings) are paid  based on the measured energy use reductions for homes they submit to PG&E as having received their services. Payments to these aggregators will be based on CalTRACK methods. The savings that the utility can claim was delivered as a result of the program will be measured through a separate EM&V process.\n\n\nCalculating \nrealized savings and realization rates\n for third-party software used to predict savings for residential efficiency project for the purposes of determining rebates. Average empirical realization rates by vendor may be used to adjust rebates on an ongoing basis and provide feedback to vendors and contractors on their relative performance.  \n\n\n\n\nNotes on key CalTRACK v1 decisions and their implications\n\n\n\n\nFixed Degree Day Base. While it is more common in industry to use a variable degree day base temperatures in monthly billing analysis, the CalTRACK technical working group decided to use fixed degree day base temperatures. This decision was made after empirical testing of both fixed and variable degree day basis showed that, for a sample of 3000 projects done from 2014-2016 in PG&E territory there was little difference in average savings between the two methods, the use of variable degree days contributed to significant differences in savings among testers. The technical working group made the determination that, since replicability was a priority for the two supported use cases, the added replicability of the fixed degree day approach made it the better choice for CalTRACK v1 methods.\n\n\nInverse Variance Weighted Means for determining aggregate savings. In determining the average savings over groups of homes, inverse variance weighted means offers a way of dealing with the important consideration that not all site-level savings estimates are equally confident. IVWM emphasizes the savings of homes with good site-level model fits, while deemphasizing sites with poor model fits. This approach comes from meta analysis and is the minimum-variance estimator of a group mean under assumptions of conditional independence. However, two challenges arise in applying IVWMs to group savings. 1) The independence assumption may not hold over areas or time periods because of structural correlation in savings. This means it may be underestimating variances due to correlation structure in the errors. Practically, this may introduce bias in the group mean depending on the correlation structure 2) larger homes will tend to have large variances and smaller homes (and gas use which is closer to 0 in many homes), leading to a potential structural underestimate of group average savings because smaller savings will have smaller variances. 3) sites with very low variances (very close to zero) can lead blow up mean savings estimates (dividing by very small numbers) and require addition checks on the final number used for programmatic purposes. Alternative weighting schemes were discussed by the technical working group, but none were ultimately selected due to similar tradeoffs faced by each weighting scheme. Users of the v1 methods should be aware of the ways that IVWMs both solves for and also creates risks to program implementation and chose the right approach and risk mitigation procedures for the program accordingly.\n\n\n\n\nImportant Caveats\n\n\n\n\nThe purpose of CalTRACK is to provide intermediate estimates of gross savings and not replace more detailed impact evaluations which focus on net savings measurement. In addition to excluding factors that are typically included in net savings estimation (selection effects, free ridership), v1 monthly methods omits corrections for population-wide factors that some consider part of gross savings (unobserved weather effects like solar exposure, macroeconomic shocks, etc.). Anyone using CalTRACK v1 methods should be aware that these potential effects on energy use are explicitly not adjusted for in v1 methods, and as a result, expose program participants and aggregators to some additional downside savings measurement risk and utilities to additional upside measurement risk. These measurement risks were weighed against the advantages in simplicity and replicability in deciding on the simple weather normalized savings approach in v1.",
            "title": "Guide to Version 1.0 Monthly Methods"
        },
        {
            "location": "/guides/v1guide/#guide-to-using-caltrack-v10-monthly-gross-savings-methods",
            "text": "The CalTRACK Version 1.0 specification provides guidance for performing monthly billing analysis for whole home weather normalized gross savings for groups of residential energy efficiency projects.  It includes data requirements and technical specifications for data preparation and cleaning, site-level billing analysis, and aggregation of site-level results.  The complete technical specification for v1 monthly methods is found in four documents   monthly/data-sources.md  describes the necessary data requirements for running CalTRACK v1  monthly/data-prep.md  describes the sequence of steps required for preparing data for analysis  /monthly/analysis.md  describes the methods use for calculating site-level savings using monthly data  monthly/aggregation.md  describes the methods use for aggregating site-level savings to group or portfolio average and total savings   The specification is intended to be done in order to ensure consistency and replicability.",
            "title": "Guide to Using CalTRACK v1.0 Monthly Gross Savings Methods"
        },
        {
            "location": "/guides/v1guide/#supported-use-cases",
            "text": "The CalTRACK v1 monthly billing analysis methods were developed to produce a reliable estimate of basic, weather normalized gross savings at the site level and weighted average gross savings for groups (or portfolios) of projects.  This definition of gross savings was developed with two specific use cases in mind, and was not intended to extend to all potential uses of monthly billing analysis.  The two use cases that CalTRACK v1 is intended to support are:   Calculating  payable savings  in the PG&E Residential Third-Party Pay-for-Performance pilot program. In this program, selected third party aggregators (organization providing services that may lead to energy savings) are paid  based on the measured energy use reductions for homes they submit to PG&E as having received their services. Payments to these aggregators will be based on CalTRACK methods. The savings that the utility can claim was delivered as a result of the program will be measured through a separate EM&V process.  Calculating  realized savings and realization rates  for third-party software used to predict savings for residential efficiency project for the purposes of determining rebates. Average empirical realization rates by vendor may be used to adjust rebates on an ongoing basis and provide feedback to vendors and contractors on their relative performance.",
            "title": "Supported Use Cases"
        },
        {
            "location": "/guides/v1guide/#notes-on-key-caltrack-v1-decisions-and-their-implications",
            "text": "Fixed Degree Day Base. While it is more common in industry to use a variable degree day base temperatures in monthly billing analysis, the CalTRACK technical working group decided to use fixed degree day base temperatures. This decision was made after empirical testing of both fixed and variable degree day basis showed that, for a sample of 3000 projects done from 2014-2016 in PG&E territory there was little difference in average savings between the two methods, the use of variable degree days contributed to significant differences in savings among testers. The technical working group made the determination that, since replicability was a priority for the two supported use cases, the added replicability of the fixed degree day approach made it the better choice for CalTRACK v1 methods.  Inverse Variance Weighted Means for determining aggregate savings. In determining the average savings over groups of homes, inverse variance weighted means offers a way of dealing with the important consideration that not all site-level savings estimates are equally confident. IVWM emphasizes the savings of homes with good site-level model fits, while deemphasizing sites with poor model fits. This approach comes from meta analysis and is the minimum-variance estimator of a group mean under assumptions of conditional independence. However, two challenges arise in applying IVWMs to group savings. 1) The independence assumption may not hold over areas or time periods because of structural correlation in savings. This means it may be underestimating variances due to correlation structure in the errors. Practically, this may introduce bias in the group mean depending on the correlation structure 2) larger homes will tend to have large variances and smaller homes (and gas use which is closer to 0 in many homes), leading to a potential structural underestimate of group average savings because smaller savings will have smaller variances. 3) sites with very low variances (very close to zero) can lead blow up mean savings estimates (dividing by very small numbers) and require addition checks on the final number used for programmatic purposes. Alternative weighting schemes were discussed by the technical working group, but none were ultimately selected due to similar tradeoffs faced by each weighting scheme. Users of the v1 methods should be aware of the ways that IVWMs both solves for and also creates risks to program implementation and chose the right approach and risk mitigation procedures for the program accordingly.",
            "title": "Notes on key CalTRACK v1 decisions and their implications"
        },
        {
            "location": "/guides/v1guide/#important-caveats",
            "text": "The purpose of CalTRACK is to provide intermediate estimates of gross savings and not replace more detailed impact evaluations which focus on net savings measurement. In addition to excluding factors that are typically included in net savings estimation (selection effects, free ridership), v1 monthly methods omits corrections for population-wide factors that some consider part of gross savings (unobserved weather effects like solar exposure, macroeconomic shocks, etc.). Anyone using CalTRACK v1 methods should be aware that these potential effects on energy use are explicitly not adjusted for in v1 methods, and as a result, expose program participants and aggregators to some additional downside savings measurement risk and utilities to additional upside measurement risk. These measurement risks were weighed against the advantages in simplicity and replicability in deciding on the simple weather normalized savings approach in v1.",
            "title": "Important Caveats"
        },
        {
            "location": "/guides/v1.1guide/",
            "text": "",
            "title": "Guide to Version 1.1 Daily Methods"
        },
        {
            "location": "/monthly/data-sources/",
            "text": "Data Sources for CalTRACK Beta Test\n\n\nTwo major types of data files are supplied for the CalTRACK Beta: project data and consumption data. This data is linked with \"cross-reference\" files that define the mapping between ID columns in the two types of files.\n\n\nThere are two types of project files, which have slightly different column types--\nAHU\n and \nAHUP\n--requiring different logic for determining baseline and reporting period dates.\n\n\nConsumption data is further broken down into five file types: 15 minutely electricity, hourly electricity, daily natural gas, monthly electricity, and monthly natural gas.\n\n\nThe beta test set uses the following files:\n\n\n\n\n\n\nProject:\n\n\nCalTRACK (AHUP) from 1_1_14__6_30_15_v2_FINAL_090816.csv\n\n\nCalTRACK (AHU) from 1_1_14__6_30_15_v2_FINAL_090816.csv\n\n\nCalTRACK (AHU) from 7_1_15__6_30_16_v2_FINAL_090816.csv\n\n\n\n\n\n\nConsumption:\n\n\n\n\n15 minutely electricity: \nIDA.15MIN.SMY1.EES25162-EHUP.20160719161716.csv\n\n\nHourly electricity: \nIDA.60MIN.SMY1.EES25162-EHUP.20160719161716\n\n\nDaily natural gas: \nEES25162_gasdy_160720.csv\n\n\nMonthly electricity: \nEES25162.ERESBL13.XPT...\n\n\nMonthly natural gas: \nEES25162.GRESBL13.XPT...\n\n\n\n\n\n\n\n\nCross-reference\n\n\n\n\nElectricity: \nEES25162_ELECINTV_XREF.csv\n\n\nNatural Gas: \nEES25162_GASINTV_XREF.csv\n\n\n\n\n\n\n\n\nThe columns of interest in these files are as follows:\n\n\nProject AHUP\n\n\n\n\n\n\n\n\nColumn Name\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nApplication No.\n\n\nProject identifier\n\n\n\n\n\n\nElectric Service ID\n\n\nID used for matching with consumption files\n\n\n\n\n\n\nGas Service ID\n\n\nID used for matching with consumption files\n\n\n\n\n\n\nFull Application Submitted\n\n\nBest proxy date for \"Work Finished\"\n\n\n\n\n\n\nFull Application Returned\n\n\nIf populated, the project got returned for correction.\n\n\n\n\n\n\nWork Start Date\n\n\nDate retrofit started, \nalways empty\n\n\n\n\n\n\nWork Finish Date\n\n\nDate retrofit ended, \nalways empty\n\n\n\n\n\n\nBuilding ZIP Code\n\n\n\n\n\n\n\n\n\n\nProject AHU\n\n\n\n\n\n\n\n\nColumn Name\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nApplication No.\n\n\nProject identifier\n\n\n\n\n\n\nElectric Service ID\n\n\nID used for matching with consumption files\n\n\n\n\n\n\nGas Service ID\n\n\nID used for matching with consumption files\n\n\n\n\n\n\nInitial Submission Date\n\n\nDate of submission of energy retrofit project paperwork (after work completed)\n\n\n\n\n\n\nWork Start Date\n\n\nDate retrofit started\n\n\n\n\n\n\nWork Finish Date\n\n\nDate retrofit ended\n\n\n\n\n\n\nBuilding ZIP Code\n\n\n\n\n\n\n\n\n\n\nConsumption\n\n\nElectricity\n\n\n\n\n\n\n\n\nColumn Name\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nSPID\n\n\nService Point ID - identifies the physical meter\n\n\n\n\n\n\nSA\n\n\nService Agreement ID - generated when a customer signs a service agreement. Can be many SAs for a single SPID. Corresponds to \nElectric Service ID\n in project files.\n\n\n\n\n\n\nUOM\n\n\nUnit of Measure (KWH or THERM)\n\n\n\n\n\n\nDIR\n\n\nDirection of electricity flow (D=delivered, R=received)\n\n\n\n\n\n\nDATE\n\n\nDay for this row of consumption data\n\n\n\n\n\n\nRS\n\n\nRate schedule of the associated SA_ID (ignored)\n\n\n\n\n\n\nAPCT\n\n\nThe actual percent of intervals are \u201cGood\u201d vs \u201cEstimated\u201d. (ignored)\n\n\n\n\n\n\nNAICS\n\n\nAssociated with activity at the premise, only for non-residential (ignored)\n\n\n\n\n\n\n00:15, etc\n\n\nConsumption data for that interval\n\n\n\n\n\n\n\n\nThere may be rows with non-zero consumption in both the R and D direction at the same timestamp (e.g. the customer both consumed power from the grid and delivered power back to the grid over the time interval).\n\n\nNatural Gas\n\n\n\n\n\n\n\n\nColumn Name\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nService Point\n\n\nService Point ID - identifies the physical meter. Corresponds to \nsp_id\n in cross-reference table.\n\n\n\n\n\n\nMeasurement Date\n\n\n\n\n\n\n\n\nTherms per day\n\n\n\n\n\n\n\n\n\n\nMonthly Consumption\n\n\nElectricity\n\n\n\n\n\n\n\n\nColumn Name\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nSA_ID\n\n\nService Agreement ID for cross-reference\n\n\n\n\n\n\nCDT__\ni\n (\ni\n [1,12])\n\n\nCurrent date the meter was read in SAS format\n\n\n\n\n\n\nPDT__\ni\n (\ni\n [1,12])\n\n\nPrevious date the meter was read\n\n\n\n\n\n\nKWH__\ni\n (\ni\n [1,12])\n\n\nUsage (KWH)\n\n\n\n\n\n\n\n\nAll other columns are ignored.\n\n\nSAS formatted dates count the number of days since Jan 1, 1960.\n\n\nDetails on rate schedules: https://www.pge.com/tariffs/ERS.SHTML\n(Currently unused but might impact \ninterpretation\n in the future.)\n\n\nNatural Gas\n\n\n\n\n\n\n\n\nColumn Name\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nSA_ID\n\n\nService Agreement ID for cross-reference\n\n\n\n\n\n\nCDT__\ni\n (\ni\n [1,12])\n\n\nCurrent date the meter was read in SAS format\n\n\n\n\n\n\nPDT__\ni\n (\ni\n [1,12])\n\n\nPrevious date the meter was read\n\n\n\n\n\n\nTHM__\ni\n (\ni\n [1,12])\n\n\nUsage (THM)\n\n\n\n\n\n\n\n\nSee notes for monthly electricity consumption above for further details.\n\n\nCross-reference\n\n\nBoth types of cross-reference files contain the same columns of interest.\n\n\n\n\n\n\n\n\nColumn Name\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nsa_id\n\n\nCorresponds to \nElectric Service ID\n or \nGas Service ID\n in projects file\n\n\n\n\n\n\nsp_id\n\n\nCorresponds to \nSPID\n in electricity consumption file and \nService Point\n in natural gas file\n\n\n\n\n\n\n\n\nOutput data format\n\n\nThe cleaned data should be is as follows:\n\n\nProjects\n\n\nAHU\n\n\n\n\n\n\n\n\nField\n\n\nValue\n\n\n\n\n\n\n\n\n\n\nproject_id\n\n\nProject file, column \nApplication No.\n\n\n\n\n\n\nzipcode\n\n\nProject file, column \nBuilding ZIP Code\n\n\n\n\n\n\nbaseline_period_end\n\n\nColumn \nWork Start Date\n unless empty, then column \nInitial Approval Date\n\n\n\n\n\n\nreporting_period_start\n\n\nColumn \nWork Finish Date\n unless empty, then column \nInitial Submission Date\n\n\n\n\n\n\n\n\nAHUP\n\n\n\n\n\n\n\n\nField\n\n\nValue\n\n\n\n\n\n\n\n\n\n\nproject_id\n\n\nProject file, column \nApplication No.\n\n\n\n\n\n\nzipcode\n\n\nProject file, column \nBuilding ZIP Code\n\n\n\n\n\n\nbaseline_period_end\n\n\nColumn \nNotice to Proceed Issued\n (Proxy for Work Start Date)\n\n\n\n\n\n\nreporting_period_start\n\n\nIf \nFull Application Returned\n is empty, column \nFull Application Submitted\n, otherwise column \nFull Application Started\n (Proxy for Work Finish Date)\n\n\n\n\n\n\n\n\nConsumption\n\n\nElectricity\n\n\n\n\n\n\n\n\nField\n\n\nValue\n\n\n\n\n\n\n\n\n\n\nproject_id\n\n\nColumn \nApplication No.\n in project file. Use the consumption cross-reference file to map the \nSPID\n column to \nElectric Service ID\n in the projects file. \nThere may be more than one matching project row\n \u2013 TBD how to handle this [1]\n\n\n\n\n\n\nstart\n\n\ncolumn \nDATE\n + column header for each time chunk\n\n\n\n\n\n\ninterpretation\n\n\nELECTRICITY_CONSUMPTION_SUPPLIED\n (\nE_C_S\n) for D direction. \nELECTRICITY_ON_SITE_GENERATION_UNCONSUMED\n (\nE_OSG_U\n) for R direction\n\n\n\n\n\n\nvalue\n\n\nValue of cell.\n\n\n\n\n\n\nestimated\n\n\nFalse\n\n\n\n\n\n\nlabel\n\n\nSA\n + \"-\" + \nSPID\n + \"-\" + \nDIR\n\n\n\n\n\n\nunit\n\n\nKWH\n\n\n\n\n\n\n\n\n[1] Only a single consumption trace matches multiple projects in the beta test data set.\n\n\nNatural Gas\n\n\n\n\n\n\n\n\nField\n\n\nValue\n\n\n\n\n\n\n\n\n\n\nproject_id\n\n\nColumn \nApplication No.\n in project file. Use the consumption cross-reference file to map the \nService Point\n column to \nGas Service ID\n in the projects file.\n\n\n\n\n\n\nstart\n\n\ncolumn \nMeasurement Date\n\n\n\n\n\n\ninterpretation\n\n\nNATURAL_GAS_CONSUMPTION_SUPPLIED\n (\nNG_C_S\n)\n\n\n\n\n\n\nvalue\n\n\nColumn \nTherms per Day\n\n\n\n\n\n\nestimated\n\n\nFalse\n\n\n\n\n\n\nlabel\n\n\nService Point\n\n\n\n\n\n\nunit\n\n\nTHM\n\n\n\n\n\n\n\n\nMonthly Consumption\n\n\nElectricity\n\n\n\n\n\n\n\n\nField\n\n\nValue\n\n\n\n\n\n\n\n\n\n\nproject_id\n\n\nColumn \nSA_ID\n corresponds to \nElectric Service ID\n in the projects file.\n\n\n\n\n\n\nstart\n\n\nparse column CDT__\ni\n for each time chunk\n\n\n\n\n\n\ninterpretation\n\n\nELECTRICITY_CONSUMPTION_SUPPLIED\n (\nE_C_S\n)\n\n\n\n\n\n\nvalue\n\n\nKWH__\ni\n\n\n\n\n\n\nestimated\n\n\nFalse\n\n\n\n\n\n\nlabel\n\n\nSA_ID\n\n\n\n\n\n\nunit\n\n\nKWH\n\n\n\n\n\n\n\n\nNatural Gas\n\n\n\n\n\n\n\n\nField\n\n\nValue\n\n\n\n\n\n\n\n\n\n\nproject_id\n\n\nColumn \nSA_ID\n corresponds to \nGas Service ID\n in the projects file.\n\n\n\n\n\n\nstart\n\n\nparse column CDT__\ni\n for each time chunk\n\n\n\n\n\n\ninterpretation\n\n\nNATURAL_GAS_CONSUMPTION_SUPPLIED\n (\nNG_C_S\n)\n\n\n\n\n\n\nvalue\n\n\nTHN__\ni\n\n\n\n\n\n\nestimated\n\n\nFalse\n\n\n\n\n\n\nlabel\n\n\nmonthly-\nSA_ID\n\n\n\n\n\n\nunit\n\n\nTHM\n\n\n\n\n\n\n\n\nWeather\n\n\nThere are three weather source files necessary for the beta test.\n\n\n\n\nCZ2010 weather normals for 86 stations in California\n\n\nMapping of zip codes to weather stations\n\n\nHourly weather station data pulled from the NOAA ISD weather data using USAF",
            "title": "Data Sources for Monthly Methods"
        },
        {
            "location": "/monthly/data-sources/#data-sources-for-caltrack-beta-test",
            "text": "Two major types of data files are supplied for the CalTRACK Beta: project data and consumption data. This data is linked with \"cross-reference\" files that define the mapping between ID columns in the two types of files.  There are two types of project files, which have slightly different column types-- AHU  and  AHUP --requiring different logic for determining baseline and reporting period dates.  Consumption data is further broken down into five file types: 15 minutely electricity, hourly electricity, daily natural gas, monthly electricity, and monthly natural gas.  The beta test set uses the following files:    Project:  CalTRACK (AHUP) from 1_1_14__6_30_15_v2_FINAL_090816.csv  CalTRACK (AHU) from 1_1_14__6_30_15_v2_FINAL_090816.csv  CalTRACK (AHU) from 7_1_15__6_30_16_v2_FINAL_090816.csv    Consumption:   15 minutely electricity:  IDA.15MIN.SMY1.EES25162-EHUP.20160719161716.csv  Hourly electricity:  IDA.60MIN.SMY1.EES25162-EHUP.20160719161716  Daily natural gas:  EES25162_gasdy_160720.csv  Monthly electricity:  EES25162.ERESBL13.XPT...  Monthly natural gas:  EES25162.GRESBL13.XPT...     Cross-reference   Electricity:  EES25162_ELECINTV_XREF.csv  Natural Gas:  EES25162_GASINTV_XREF.csv     The columns of interest in these files are as follows:  Project AHUP     Column Name  Description      Application No.  Project identifier    Electric Service ID  ID used for matching with consumption files    Gas Service ID  ID used for matching with consumption files    Full Application Submitted  Best proxy date for \"Work Finished\"    Full Application Returned  If populated, the project got returned for correction.    Work Start Date  Date retrofit started,  always empty    Work Finish Date  Date retrofit ended,  always empty    Building ZIP Code      Project AHU     Column Name  Description      Application No.  Project identifier    Electric Service ID  ID used for matching with consumption files    Gas Service ID  ID used for matching with consumption files    Initial Submission Date  Date of submission of energy retrofit project paperwork (after work completed)    Work Start Date  Date retrofit started    Work Finish Date  Date retrofit ended    Building ZIP Code      Consumption  Electricity     Column Name  Description      SPID  Service Point ID - identifies the physical meter    SA  Service Agreement ID - generated when a customer signs a service agreement. Can be many SAs for a single SPID. Corresponds to  Electric Service ID  in project files.    UOM  Unit of Measure (KWH or THERM)    DIR  Direction of electricity flow (D=delivered, R=received)    DATE  Day for this row of consumption data    RS  Rate schedule of the associated SA_ID (ignored)    APCT  The actual percent of intervals are \u201cGood\u201d vs \u201cEstimated\u201d. (ignored)    NAICS  Associated with activity at the premise, only for non-residential (ignored)    00:15, etc  Consumption data for that interval     There may be rows with non-zero consumption in both the R and D direction at the same timestamp (e.g. the customer both consumed power from the grid and delivered power back to the grid over the time interval).  Natural Gas     Column Name  Description      Service Point  Service Point ID - identifies the physical meter. Corresponds to  sp_id  in cross-reference table.    Measurement Date     Therms per day      Monthly Consumption  Electricity     Column Name  Description      SA_ID  Service Agreement ID for cross-reference    CDT__ i  ( i  [1,12])  Current date the meter was read in SAS format    PDT__ i  ( i  [1,12])  Previous date the meter was read    KWH__ i  ( i  [1,12])  Usage (KWH)     All other columns are ignored.  SAS formatted dates count the number of days since Jan 1, 1960.  Details on rate schedules: https://www.pge.com/tariffs/ERS.SHTML\n(Currently unused but might impact  interpretation  in the future.)  Natural Gas     Column Name  Description      SA_ID  Service Agreement ID for cross-reference    CDT__ i  ( i  [1,12])  Current date the meter was read in SAS format    PDT__ i  ( i  [1,12])  Previous date the meter was read    THM__ i  ( i  [1,12])  Usage (THM)     See notes for monthly electricity consumption above for further details.  Cross-reference  Both types of cross-reference files contain the same columns of interest.     Column Name  Description      sa_id  Corresponds to  Electric Service ID  or  Gas Service ID  in projects file    sp_id  Corresponds to  SPID  in electricity consumption file and  Service Point  in natural gas file",
            "title": "Data Sources for CalTRACK Beta Test"
        },
        {
            "location": "/monthly/data-sources/#output-data-format",
            "text": "The cleaned data should be is as follows:  Projects  AHU     Field  Value      project_id  Project file, column  Application No.    zipcode  Project file, column  Building ZIP Code    baseline_period_end  Column  Work Start Date  unless empty, then column  Initial Approval Date    reporting_period_start  Column  Work Finish Date  unless empty, then column  Initial Submission Date     AHUP     Field  Value      project_id  Project file, column  Application No.    zipcode  Project file, column  Building ZIP Code    baseline_period_end  Column  Notice to Proceed Issued  (Proxy for Work Start Date)    reporting_period_start  If  Full Application Returned  is empty, column  Full Application Submitted , otherwise column  Full Application Started  (Proxy for Work Finish Date)     Consumption  Electricity     Field  Value      project_id  Column  Application No.  in project file. Use the consumption cross-reference file to map the  SPID  column to  Electric Service ID  in the projects file.  There may be more than one matching project row  \u2013 TBD how to handle this [1]    start  column  DATE  + column header for each time chunk    interpretation  ELECTRICITY_CONSUMPTION_SUPPLIED  ( E_C_S ) for D direction.  ELECTRICITY_ON_SITE_GENERATION_UNCONSUMED  ( E_OSG_U ) for R direction    value  Value of cell.    estimated  False    label  SA  + \"-\" +  SPID  + \"-\" +  DIR    unit  KWH     [1] Only a single consumption trace matches multiple projects in the beta test data set.  Natural Gas     Field  Value      project_id  Column  Application No.  in project file. Use the consumption cross-reference file to map the  Service Point  column to  Gas Service ID  in the projects file.    start  column  Measurement Date    interpretation  NATURAL_GAS_CONSUMPTION_SUPPLIED  ( NG_C_S )    value  Column  Therms per Day    estimated  False    label  Service Point    unit  THM     Monthly Consumption  Electricity     Field  Value      project_id  Column  SA_ID  corresponds to  Electric Service ID  in the projects file.    start  parse column CDT__ i  for each time chunk    interpretation  ELECTRICITY_CONSUMPTION_SUPPLIED  ( E_C_S )    value  KWH__ i    estimated  False    label  SA_ID    unit  KWH     Natural Gas     Field  Value      project_id  Column  SA_ID  corresponds to  Gas Service ID  in the projects file.    start  parse column CDT__ i  for each time chunk    interpretation  NATURAL_GAS_CONSUMPTION_SUPPLIED  ( NG_C_S )    value  THN__ i    estimated  False    label  monthly- SA_ID    unit  THM",
            "title": "Output data format"
        },
        {
            "location": "/monthly/data-sources/#weather",
            "text": "There are three weather source files necessary for the beta test.   CZ2010 weather normals for 86 stations in California  Mapping of zip codes to weather stations  Hourly weather station data pulled from the NOAA ISD weather data using USAF",
            "title": "Weather"
        },
        {
            "location": "/monthly/data-prep/",
            "text": "CalTRACK Beta Test Data Preparation Guidelines\n\n\nData Cleaning and Quality Checks for the CalTRACK Beta Test will consist of three main tasks\n\n\n\n\nExplore raw data sources with non-standard summary statistics\n\n\nPerform data cleaning and integration procedures on raw data\n\n\nCalculate and report prepared data summary statistics\n\n\n\n\n=======\n\n\nData Cleaning and Munging\n\n\nA number of cleaning steps are necessary to use the raw data.\n\n\nProject Filtering\n\n\n\n\nFilter out any projects for which there is not an \nsa_id\n found in the cross reference files for either \nElectric Service ID\n or \nGas Service ID\n\n\n\n\nDeduplication\n\n\n\n\n\n\nIf a home appears multiple times within a project database, and the project dates are the same the most complete record for that home will be the record used in CalTRACK\n\n\n\n\n\n\nIf a home appears multiple times within a project database and the project dates differ because there are multiple measures installed associated with the same incentive program, the start date of the intervention will be the earliest of the project start dates across projects and the end date for the intervention will be the latest of the project end dates\n\n\n\n\n\n\nThere are a small number of duplicate traces \u2013 consumption traces with unique SAs (though identical SPIDs) and identical consumption data over the time interval.\n\n\n\n\n\n\nIf two duplicate records have identical consumption traces and date ranges, drop one at random\n\n\n\n\n\n\nIf two duplicate records have identical consumption traces but different date ranges select the more complete record having more dates. If the dates are contiguous, or there are overlapping dates with the same usage values, combine the two traces into a single trace.\n\n\n\n\n\n\nIf they have the same date ranges, but different usage values, the project is flagged and the record is excluded from the sample.\n\n\n\n\n\n\nCreating Work Start and Work End dates from raw project data\n\n\nThe dates for CalTRACK Beta Test come from the following Final CalTRACK files provided by Build it Green:\n\n\nCalTRACK (AHU) from 1_1_14__6_30_15_v2_FINAL_090816.csv\n\n\nCalTRACK (AHU) from 7_1_15__6_30_16_v2_FINAL_090816.csv\n\n\nCalTRACK (AHUP) from 1_1_14__6_30_15_v2_FINAL_090816.csv\n\n\nThe following rules are used for determining \nwork start dates\n:\n\n\n\n\n\n\nFrom the updated \u2018AHUP\u2019 file \u201cCalTRACK (AHUP) from 1_1_14__6_30_15_v2_FINAL_090816\u201d\n\n\n\n\nColumn G \u2013 \u201cNotice to Proceed Issued\u201d (Best Proxy for \u2018Work Start\u2019)\n\n\n\n\n\n\n\n\nFrom the updated \u2018AHU with correlated XML files\u2019 file \u201cCalTRACK (AHU) from 1_1_14__6_30_15_v2_FINAL_090816\u201d\n\n\n\n\nColumn F \u2013 \u201cInitial Approval Date\u201d (Best Proxy for \u2018Work Start\u2019)\n\n\n\n\n\n\n\n\nFrom the updated \u2018AHU Control Group (no XML files)\u2019 file \u201cCalTRACK (AHU) from 7_1_15__6_30_16_v2_FINAL_090816\u201d\n\n\n\n\nColumn F \u2013 \u201cInitial Approval Date\u201d (Best Proxy for \u2018Work Start\u2019)\n\n\n\n\n\n\n\n\nThe following rules are used for determining \nwork end dates\n:\n\n\n\n\n\n\nFrom the updated \u2018AHUP\u2019 file \u201cCalTRACK (AHUP) from 1_1_14__6_30_15_v2_FINAL_090816\u201d\n\n\n\n\nColumn H \u2013 \u201cFull Application Started\u201d (Best Proxy for \u2018Work Finished\u2019)\n\n\nPlease Note \u2013 Column J \u2013 \u201cFull Application Submitted\u201d can represent a better proxy for \u2018Work Finished\u2019, if the project was not returned for correction(s) during the QA review process (i.e., if there is a date in Column I \u2013 \u201cFull Application Returned\u201d, it got returned for correction). If a project gets returned for correction, then Column J\u2019s \u201cFull Application Submitted\u201d date becomes, effectively, a \u201cFull Application \u2018Re-submitted\u2019\u201d, and no longer represents a good proxy for \u2018Work Finished\u2019 as it is +1-10 Days (or more) removed at that point.\n\n\n\n\n\n\n\n\nFrom the updated \u2018AHU with correlated XML files\u2019 file \u201cCalTRACK (AHU) from 1_1_14__6_30_15_v2_FINAL_090816\u201d\n\n\n\n\nColumn G \u2013 \u201cInitial Submission Date\u201d (Best Proxy for \u2018Work Finished\u2019)\n\n\n\n\n\n\n\n\nFrom the updated \u2018AHU Control Group (no XML files)\u2019 file \u201cCalTRACK (AHU) from 7_1_15__6_30_16_v2_FINAL_090816\u201d\n\n\n\n\nColumn G \u2013 \u201cInitial Submission Date\u201d (Best Proxy for \u2018Work Finished\u2019)\n\n\n\n\n\n\n\n\nMissing Values & Imputation\n\n\nWeather\n\n\n\n\n\n\nHourly\n\n\n\n\n\n\nHourly weather data from GSOD will not be imputed\n\n\n\n\n\n\nDays with more than 5 contiguous hours of missing data will be thrown out\n\n\n\n\n\n\n\n\n\n\nDaily\n\n\n\n\n\n\nGSOD daily averages will not be imputed\n\n\n\n\n\n\nMonths with more than than 3 missing days will be thrown out\n\n\n\n\n\n\n\n\n\n\nUsage\n\n\n\n\n\n\nMissing values where the cumulative value is in the following period, the cumulative number of days between the two periods will be used to generate the UPD for that period\n\n\n\n\n\n\nMissing usage values with no cumulative amount in the following period will be counted against data sufficiency requirements\n\n\n\n\n\n\nHomes with Net Metering will be dropped from the analysis\n\n\n\n\n\n\nEstimated values & deletion\n\n\n\n\nEstimated usage data will be used for estimation, but estimation flags will be added to the post-estimation\n\n\n\n\nExtreme values\n\n\nUsage\n\n\n\n\n\n\nNegative values or values with reverse direction of flow will be treated as missing and count against sufficiency criterion. The account will also be flag within CalTRACK for possible net metering if it does not currently contain a net metering flag\n\n\n\n\n\n\nIt is assumed at all IOUs comply with DASMMD.\n\n\n\n\n\n\nAMI data will have the DASMMD pass/fail criterion rerun, with failing values coded as missing. ((highest peak - third highest peak)/third highest peak) <= 1.8\n\n\n\n\n\n\nProject Data\n\n\n\n\n\n\nExtreme project lengths (gap between project start date and project end date longer than 3 months) will be treated as true and impact estimation only through data sufficiency requirements.\n\n\n\n\n\n\nFiles without project start dates are thrown out\n\n\n\n\n\n\nSum Check\n\n\nIf both monthly and AMI data are available for a home, CalTRACK will run a sumcheck and use the DASMMD criterion for pass/fail. If it fails, the home is flagged and treated as having missing usage data so no estimation is run on it.\n\n\nMiscoded values\n\n\n\n\nMiscoded strings in project data will be deduplicated and matched (fuzzily) to closest value\n\n\n\n\nMiscoded dates\n\n\n\n\n\n\nImplausible day values (>31) will be coded as the beginning of month if project start date and end of month if project end date so that the entire month included in the intervention window\n\n\n\n\n\n\nImplausible month and year values will be flagged and that home not included in estimation.\n\n\n\n\n\n\nData sufficiency\n\n\nUsage (Monthly)\n\n\n\n\n\n\n12 complete months pre-retrofit for monthly billing data to qualify for estimation or 24 months with up to 2 missing values from different, non-contiguous months\n\n\n\n\n\n\nPost retrofit data sufficiency for estimation will be dealt with in post-estimation model fit criterion\n\n\n\n\n\n\nTotal annual savings estimates will require 12 months post-retrofit\n\n\n\n\n\n\nUsage (AMI)\n\n\n\n\n\n\n12 months pre-retrofit\n\n\n\n\n\n\nPost retrofit data sufficiency for estimation will be dealt with in post-estimation model fit criterion\n\n\n\n\n\n\nTotal annual savings estimates will require 12 months post-retrofit\n\n\n\n\n\n\nWeather\n\n\n\n\nThere should not be problems with data sufficiency for weather\n\n\n\n\nProject or Home Characteristics\n\n\n\n\nExclude homes with PV for whom solar production data is not available\n\n\n\n\nValue Adjustments (if values change during performance period)\n\n\nUsage\n\n\n\n\n\n\nUse most up-to-date meter read for\n\n\n\n\n\n\nLog prior values and prior estimates\n\n\n\n\n\n\nProject data\n\n\n\n\nUse most up to date values for estimation\n\n\n\n\nWeather data\n\n\n\n\nUse most recent daily weather value for estimation\n\n\n\n\nData Integration\n\n\n\n\nMatching project data\n\n\nMatching will be done using the cross-reference files contain the sa_id and sp_id mapping between the project data id fields and the consumption data id fields.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nColumn Name\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nsa_id\n\n\nCorresponds to \nElectric Service ID\n or \nGas Service ID\n in projects file\n\n\n\n\n\n\nsp_id\n\n\nCorresponds to \nSPID\n in electricity consumption file and \nService Point\n in natural gas file\n\n\n\n\n\n\n\n\n\n\n\n\nMatching weather stations to projects\n\n\n\n\nWeather station mapping will be done using the 86 station standard mapping of zip code to CZ2010 weather files provided in the \n/data-source\n directory\n\n\n\n\n\n\n\n\nUnmatched data\n\n\n\n\nProjects that are unmatched to usage data will be listed in CalTRACK for data integrity reporting, but will not be included in any estimation procedures and will not have estimated savings\n\n\n\n\n\n\n\n\nPrepared Data Summary Statistics for Comparison\n\n\nTo ensure that beta testers are working with the same datasets and that we are characterizing the datasets in a way that is helpful for other members of the technical working group as well as the general public, each beta tester will generate a set of summary statistics and perform a set of data quality checks that can be shared with the larger group through csvs saved to this repository. There will be four cleaned data reports generated by each beta tester: a project data summary file, an hourly electric summary file, a daily gas summary file, and data integration summary file. Each file will be a .csv and will have the following general format:\n\n\n\n\n\n\n\n\nSummary Stat\n\n\nValue\n\n\n\n\n\n\n\n\n\n\nNumber of Observations\n\n\n4321\n\n\n\n\n\n\nNumber of Fields\n\n\n4\n\n\n\n\n\n\nMin Work Start Date\n\n\nJan, 21, 2014\n\n\n\n\n\n\nMax Work Start Date\n\n\nJune 23, 2015\n\n\n\n\n\n\n\n\nThe specifics for each summary file are below:\n\n\nCombined Project Data Summary File\n\n\nOutput Filename: \nproject_data_summary_NAME_OF_TESTER.csv\n\n\nApply project filtering rules, then compute the following statistics on the filtered list of projects:\n\n\nIncluded Summary statistics\n\n\n\n\nTotal number of records\n\n\nNumber of unique project electric SAIDs\n\n\nTop 10 zip codes by count\n\n\nSort by descending counts,\n\n\nFor identical counts, sort by ascending zip code\n\n\n\n\n\n\nBottom ten zip codes by count\n\n\nSort by ascending counts,\n\n\nFor identical counts, sort by ascending zip code\n\n\n\n\n\n\nMin Work Start Date\n\n\nMax Work Start Date\n\n\nAverage Work Start Date\n\n\nWork Start Date 10th percentile value\n\n\nWork Start Date 20th percentile value\n\n\nWork Start Date 30th percentile value\n\n\nWork Start Date 40th percentile value\n\n\nWork Start Date 50th percentile value\n\n\nWork Start Date 60th percentile value\n\n\nWork Start Date 70th percentile value\n\n\nWork Start Date 80th percentile value\n\n\nWork Start Date 90th percentile value\n\n\nMin Work End Date\n\n\nMax Work End Date\n\n\nAverage Work End Date\n\n\nWork End Date 10th percentile value\n\n\nWork End Date 20th percentile value\n\n\nWork End Date 30th percentile value\n\n\nWork End Date 40th percentile value\n\n\nWork End Date 50th percentile value\n\n\nWork End Date 60th percentile value\n\n\nWork End Date 70th percentile value\n\n\nWork End Date 80th percentile value\n\n\nWork End Date 90th percentile value\n\n\n\n\nFormatting\n\n\n\n\nTruncate dates to the nearest day\n\n\nFormat dates as YYYY-MM-DD\n\n\n\n\nPrepared Hourly Electricity Data Summary File\n\n\nOutput Filename: \nhourly_electricity_data_summary_NAME_OF_TESTER.csv\n\n\nIncluded Summary statistics\n\n\n\n\nTotal number of usage records\n\n\nNumber of unique SPIDs\n\n\nTotal number of missing hours across all usage records\n\n\nTotal number of estimated hours across all usage records\n\n\nNumber of net metering accounts dropped from sample\n\n\nTop 10 users by total use\n\n\nBottom 10 users by total use\n\n\nMin Datetime stamp across all usage records\n\n\nMax Datetime stamp across all usage records\n\n\nAverage Datetime stamp across all usage records\n\n\nMin use across all usage records\n\n\nMax use across all usage records\n\n\nAverage use across all usage records\n\n\nUse across all usage records 10th percentile value\n\n\nUse across all usage records 20th percentile value\n\n\nUse across all usage records 30th percentile value\n\n\nUse across all usage records 40th percentile value\n\n\nUse across all usage records 50th percentile value\n\n\nUse across all usage records 60th percentile value\n\n\nUse across all usage records 70th percentile value\n\n\nUse across all usage records 80th percentile value\n\n\nUse across all usage records 90th percentile value\n\n\n\n\nPrepared Daily Gas Data Summary File\n\n\nOutput Filename: \ndaily_gas_data_summary_NAME_OF_TESTER.csv\n\n\nIncluded Summary statistics\n\n\n\n\nTotal number of records\n\n\nNumber of unique SPIDs\n\n\nTotal number of missing days across all usage records\n\n\nTotal number of estimated hours across all usage records\n\n\nTop 10 users by total use\n\n\nBottom 10 users by total use\n\n\nMin Datetime stamp across all usage records\n\n\nMax Datetime stamp across all usage records\n\n\nAverage Datetime stamp across all usage records\n\n\nMin use across all usage records\n\n\nMax use across all usage records\n\n\nAverage use across all usage records\n\n\nUse across all usage records 10th percentile value\n\n\nUse across all usage records 20th percentile value\n\n\nUse across all usage records 30th percentile value\n\n\nUse across all usage records 40th percentile value\n\n\nUse across all usage records 50th percentile value\n\n\nUse across all usage records 60th percentile value\n\n\nUse across all usage records 70th percentile value\n\n\nUse across all usage records 80th percentile value\n\n\nUse across all usage records 90th percentile value\n\n\n\n\nData Integration Summary File\n\n\nOutput Filename: \ndata_integration_electricity_summary_NAME_OF_TESTER.csv\n\n\nIncluded Summary statistics\n\n\n\n\nNumber of unique SAIDs in joined data\n\n\nNumber of unique SPIDs in hourly electricity data\n\n\nNumber of unique SPIDs in monthly electricity data\n\n\nNumber of unique SPIDs in the electricity cross reference dataset\n\n\nNumber of unique SPIDS in the gas cross reference dataset\n\n\nNumber of unique SAIDs in the electricity cross reference dataset\n\n\nNumber of unique SAIDs in the gas cross reference dataset\n\n\n% Records successfully matched between hourly electricity data to project data\n\n\nFrequency counts of invalid rows from missing sa_id\n\n\nFrequency counts of invalid rows from missing sp_id\n\n\n\n\n\n\n% Records successfully matched between daily gas data to project data\n\n\nFrequency counts of invalid rows from missing sa_id\n\n\nFrequency counts of invalid rows from missing sp_id\n\n\n\n\n\n\n% Records successfully matched between hourly electricity data to project data\n\n\nFrequency counts of invalid rows from missing sa_id\n\n\nFrequency counts of invalid rows from missing sp_id\n\n\n\n\n\n\n% Records successfully matched between monthly electricity data to project data\n\n\nFrequency counts of invalid rows from missing sa_id\n\n\nFrequency counts of invalid rows from missing sp_id",
            "title": "Data Preparation for Monthly Methods"
        },
        {
            "location": "/monthly/data-prep/#caltrack-beta-test-data-preparation-guidelines",
            "text": "Data Cleaning and Quality Checks for the CalTRACK Beta Test will consist of three main tasks   Explore raw data sources with non-standard summary statistics  Perform data cleaning and integration procedures on raw data  Calculate and report prepared data summary statistics   =======",
            "title": "CalTRACK Beta Test Data Preparation Guidelines"
        },
        {
            "location": "/monthly/data-prep/#data-cleaning-and-munging",
            "text": "A number of cleaning steps are necessary to use the raw data.",
            "title": "Data Cleaning and Munging"
        },
        {
            "location": "/monthly/data-prep/#project-filtering",
            "text": "Filter out any projects for which there is not an  sa_id  found in the cross reference files for either  Electric Service ID  or  Gas Service ID",
            "title": "Project Filtering"
        },
        {
            "location": "/monthly/data-prep/#deduplication",
            "text": "If a home appears multiple times within a project database, and the project dates are the same the most complete record for that home will be the record used in CalTRACK    If a home appears multiple times within a project database and the project dates differ because there are multiple measures installed associated with the same incentive program, the start date of the intervention will be the earliest of the project start dates across projects and the end date for the intervention will be the latest of the project end dates    There are a small number of duplicate traces \u2013 consumption traces with unique SAs (though identical SPIDs) and identical consumption data over the time interval.    If two duplicate records have identical consumption traces and date ranges, drop one at random    If two duplicate records have identical consumption traces but different date ranges select the more complete record having more dates. If the dates are contiguous, or there are overlapping dates with the same usage values, combine the two traces into a single trace.    If they have the same date ranges, but different usage values, the project is flagged and the record is excluded from the sample.",
            "title": "Deduplication"
        },
        {
            "location": "/monthly/data-prep/#creating-work-start-and-work-end-dates-from-raw-project-data",
            "text": "The dates for CalTRACK Beta Test come from the following Final CalTRACK files provided by Build it Green:  CalTRACK (AHU) from 1_1_14__6_30_15_v2_FINAL_090816.csv  CalTRACK (AHU) from 7_1_15__6_30_16_v2_FINAL_090816.csv  CalTRACK (AHUP) from 1_1_14__6_30_15_v2_FINAL_090816.csv  The following rules are used for determining  work start dates :    From the updated \u2018AHUP\u2019 file \u201cCalTRACK (AHUP) from 1_1_14__6_30_15_v2_FINAL_090816\u201d   Column G \u2013 \u201cNotice to Proceed Issued\u201d (Best Proxy for \u2018Work Start\u2019)     From the updated \u2018AHU with correlated XML files\u2019 file \u201cCalTRACK (AHU) from 1_1_14__6_30_15_v2_FINAL_090816\u201d   Column F \u2013 \u201cInitial Approval Date\u201d (Best Proxy for \u2018Work Start\u2019)     From the updated \u2018AHU Control Group (no XML files)\u2019 file \u201cCalTRACK (AHU) from 7_1_15__6_30_16_v2_FINAL_090816\u201d   Column F \u2013 \u201cInitial Approval Date\u201d (Best Proxy for \u2018Work Start\u2019)     The following rules are used for determining  work end dates :    From the updated \u2018AHUP\u2019 file \u201cCalTRACK (AHUP) from 1_1_14__6_30_15_v2_FINAL_090816\u201d   Column H \u2013 \u201cFull Application Started\u201d (Best Proxy for \u2018Work Finished\u2019)  Please Note \u2013 Column J \u2013 \u201cFull Application Submitted\u201d can represent a better proxy for \u2018Work Finished\u2019, if the project was not returned for correction(s) during the QA review process (i.e., if there is a date in Column I \u2013 \u201cFull Application Returned\u201d, it got returned for correction). If a project gets returned for correction, then Column J\u2019s \u201cFull Application Submitted\u201d date becomes, effectively, a \u201cFull Application \u2018Re-submitted\u2019\u201d, and no longer represents a good proxy for \u2018Work Finished\u2019 as it is +1-10 Days (or more) removed at that point.     From the updated \u2018AHU with correlated XML files\u2019 file \u201cCalTRACK (AHU) from 1_1_14__6_30_15_v2_FINAL_090816\u201d   Column G \u2013 \u201cInitial Submission Date\u201d (Best Proxy for \u2018Work Finished\u2019)     From the updated \u2018AHU Control Group (no XML files)\u2019 file \u201cCalTRACK (AHU) from 7_1_15__6_30_16_v2_FINAL_090816\u201d   Column G \u2013 \u201cInitial Submission Date\u201d (Best Proxy for \u2018Work Finished\u2019)",
            "title": "Creating Work Start and Work End dates from raw project data"
        },
        {
            "location": "/monthly/data-prep/#missing-values-imputation",
            "text": "Weather    Hourly    Hourly weather data from GSOD will not be imputed    Days with more than 5 contiguous hours of missing data will be thrown out      Daily    GSOD daily averages will not be imputed    Months with more than than 3 missing days will be thrown out      Usage    Missing values where the cumulative value is in the following period, the cumulative number of days between the two periods will be used to generate the UPD for that period    Missing usage values with no cumulative amount in the following period will be counted against data sufficiency requirements    Homes with Net Metering will be dropped from the analysis",
            "title": "Missing Values &amp; Imputation"
        },
        {
            "location": "/monthly/data-prep/#estimated-values-deletion",
            "text": "Estimated usage data will be used for estimation, but estimation flags will be added to the post-estimation",
            "title": "Estimated values &amp; deletion"
        },
        {
            "location": "/monthly/data-prep/#extreme-values",
            "text": "Usage    Negative values or values with reverse direction of flow will be treated as missing and count against sufficiency criterion. The account will also be flag within CalTRACK for possible net metering if it does not currently contain a net metering flag    It is assumed at all IOUs comply with DASMMD.    AMI data will have the DASMMD pass/fail criterion rerun, with failing values coded as missing. ((highest peak - third highest peak)/third highest peak) <= 1.8    Project Data    Extreme project lengths (gap between project start date and project end date longer than 3 months) will be treated as true and impact estimation only through data sufficiency requirements.    Files without project start dates are thrown out",
            "title": "Extreme values"
        },
        {
            "location": "/monthly/data-prep/#sum-check",
            "text": "If both monthly and AMI data are available for a home, CalTRACK will run a sumcheck and use the DASMMD criterion for pass/fail. If it fails, the home is flagged and treated as having missing usage data so no estimation is run on it.",
            "title": "Sum Check"
        },
        {
            "location": "/monthly/data-prep/#miscoded-values",
            "text": "Miscoded strings in project data will be deduplicated and matched (fuzzily) to closest value",
            "title": "Miscoded values"
        },
        {
            "location": "/monthly/data-prep/#miscoded-dates",
            "text": "Implausible day values (>31) will be coded as the beginning of month if project start date and end of month if project end date so that the entire month included in the intervention window    Implausible month and year values will be flagged and that home not included in estimation.",
            "title": "Miscoded dates"
        },
        {
            "location": "/monthly/data-prep/#data-sufficiency",
            "text": "Usage (Monthly)    12 complete months pre-retrofit for monthly billing data to qualify for estimation or 24 months with up to 2 missing values from different, non-contiguous months    Post retrofit data sufficiency for estimation will be dealt with in post-estimation model fit criterion    Total annual savings estimates will require 12 months post-retrofit    Usage (AMI)    12 months pre-retrofit    Post retrofit data sufficiency for estimation will be dealt with in post-estimation model fit criterion    Total annual savings estimates will require 12 months post-retrofit    Weather   There should not be problems with data sufficiency for weather",
            "title": "Data sufficiency"
        },
        {
            "location": "/monthly/data-prep/#project-or-home-characteristics",
            "text": "Exclude homes with PV for whom solar production data is not available",
            "title": "Project or Home Characteristics"
        },
        {
            "location": "/monthly/data-prep/#value-adjustments-if-values-change-during-performance-period",
            "text": "",
            "title": "Value Adjustments (if values change during performance period)"
        },
        {
            "location": "/monthly/data-prep/#usage",
            "text": "Use most up-to-date meter read for    Log prior values and prior estimates    Project data   Use most up to date values for estimation   Weather data   Use most recent daily weather value for estimation",
            "title": "Usage"
        },
        {
            "location": "/monthly/data-prep/#data-integration",
            "text": "Matching project data  Matching will be done using the cross-reference files contain the sa_id and sp_id mapping between the project data id fields and the consumption data id fields.        Column Name  Description      sa_id  Corresponds to  Electric Service ID  or  Gas Service ID  in projects file    sp_id  Corresponds to  SPID  in electricity consumption file and  Service Point  in natural gas file       Matching weather stations to projects   Weather station mapping will be done using the 86 station standard mapping of zip code to CZ2010 weather files provided in the  /data-source  directory     Unmatched data   Projects that are unmatched to usage data will be listed in CalTRACK for data integrity reporting, but will not be included in any estimation procedures and will not have estimated savings",
            "title": "Data Integration"
        },
        {
            "location": "/monthly/data-prep/#prepared-data-summary-statistics-for-comparison",
            "text": "To ensure that beta testers are working with the same datasets and that we are characterizing the datasets in a way that is helpful for other members of the technical working group as well as the general public, each beta tester will generate a set of summary statistics and perform a set of data quality checks that can be shared with the larger group through csvs saved to this repository. There will be four cleaned data reports generated by each beta tester: a project data summary file, an hourly electric summary file, a daily gas summary file, and data integration summary file. Each file will be a .csv and will have the following general format:     Summary Stat  Value      Number of Observations  4321    Number of Fields  4    Min Work Start Date  Jan, 21, 2014    Max Work Start Date  June 23, 2015     The specifics for each summary file are below:",
            "title": "Prepared Data Summary Statistics for Comparison"
        },
        {
            "location": "/monthly/data-prep/#combined-project-data-summary-file",
            "text": "Output Filename:  project_data_summary_NAME_OF_TESTER.csv  Apply project filtering rules, then compute the following statistics on the filtered list of projects:",
            "title": "Combined Project Data Summary File"
        },
        {
            "location": "/monthly/data-prep/#included-summary-statistics",
            "text": "Total number of records  Number of unique project electric SAIDs  Top 10 zip codes by count  Sort by descending counts,  For identical counts, sort by ascending zip code    Bottom ten zip codes by count  Sort by ascending counts,  For identical counts, sort by ascending zip code    Min Work Start Date  Max Work Start Date  Average Work Start Date  Work Start Date 10th percentile value  Work Start Date 20th percentile value  Work Start Date 30th percentile value  Work Start Date 40th percentile value  Work Start Date 50th percentile value  Work Start Date 60th percentile value  Work Start Date 70th percentile value  Work Start Date 80th percentile value  Work Start Date 90th percentile value  Min Work End Date  Max Work End Date  Average Work End Date  Work End Date 10th percentile value  Work End Date 20th percentile value  Work End Date 30th percentile value  Work End Date 40th percentile value  Work End Date 50th percentile value  Work End Date 60th percentile value  Work End Date 70th percentile value  Work End Date 80th percentile value  Work End Date 90th percentile value   Formatting   Truncate dates to the nearest day  Format dates as YYYY-MM-DD",
            "title": "Included Summary statistics"
        },
        {
            "location": "/monthly/data-prep/#prepared-hourly-electricity-data-summary-file",
            "text": "Output Filename:  hourly_electricity_data_summary_NAME_OF_TESTER.csv",
            "title": "Prepared Hourly Electricity Data Summary File"
        },
        {
            "location": "/monthly/data-prep/#included-summary-statistics_1",
            "text": "Total number of usage records  Number of unique SPIDs  Total number of missing hours across all usage records  Total number of estimated hours across all usage records  Number of net metering accounts dropped from sample  Top 10 users by total use  Bottom 10 users by total use  Min Datetime stamp across all usage records  Max Datetime stamp across all usage records  Average Datetime stamp across all usage records  Min use across all usage records  Max use across all usage records  Average use across all usage records  Use across all usage records 10th percentile value  Use across all usage records 20th percentile value  Use across all usage records 30th percentile value  Use across all usage records 40th percentile value  Use across all usage records 50th percentile value  Use across all usage records 60th percentile value  Use across all usage records 70th percentile value  Use across all usage records 80th percentile value  Use across all usage records 90th percentile value",
            "title": "Included Summary statistics"
        },
        {
            "location": "/monthly/data-prep/#prepared-daily-gas-data-summary-file",
            "text": "Output Filename:  daily_gas_data_summary_NAME_OF_TESTER.csv",
            "title": "Prepared Daily Gas Data Summary File"
        },
        {
            "location": "/monthly/data-prep/#included-summary-statistics_2",
            "text": "Total number of records  Number of unique SPIDs  Total number of missing days across all usage records  Total number of estimated hours across all usage records  Top 10 users by total use  Bottom 10 users by total use  Min Datetime stamp across all usage records  Max Datetime stamp across all usage records  Average Datetime stamp across all usage records  Min use across all usage records  Max use across all usage records  Average use across all usage records  Use across all usage records 10th percentile value  Use across all usage records 20th percentile value  Use across all usage records 30th percentile value  Use across all usage records 40th percentile value  Use across all usage records 50th percentile value  Use across all usage records 60th percentile value  Use across all usage records 70th percentile value  Use across all usage records 80th percentile value  Use across all usage records 90th percentile value",
            "title": "Included Summary statistics"
        },
        {
            "location": "/monthly/data-prep/#data-integration-summary-file",
            "text": "Output Filename:  data_integration_electricity_summary_NAME_OF_TESTER.csv",
            "title": "Data Integration Summary File"
        },
        {
            "location": "/monthly/data-prep/#included-summary-statistics_3",
            "text": "Number of unique SAIDs in joined data  Number of unique SPIDs in hourly electricity data  Number of unique SPIDs in monthly electricity data  Number of unique SPIDs in the electricity cross reference dataset  Number of unique SPIDS in the gas cross reference dataset  Number of unique SAIDs in the electricity cross reference dataset  Number of unique SAIDs in the gas cross reference dataset  % Records successfully matched between hourly electricity data to project data  Frequency counts of invalid rows from missing sa_id  Frequency counts of invalid rows from missing sp_id    % Records successfully matched between daily gas data to project data  Frequency counts of invalid rows from missing sa_id  Frequency counts of invalid rows from missing sp_id    % Records successfully matched between hourly electricity data to project data  Frequency counts of invalid rows from missing sa_id  Frequency counts of invalid rows from missing sp_id    % Records successfully matched between monthly electricity data to project data  Frequency counts of invalid rows from missing sa_id  Frequency counts of invalid rows from missing sp_id",
            "title": "Included Summary statistics"
        },
        {
            "location": "/monthly/analysis/",
            "text": "CalTRACK Site-level Monthly Gross Savings Estimation Technical Guideline\n\n\n\n\nMethodological Overview\n\n\nSite-level gross savings using monthly billing data (both electricity and gas) will use a two-stage estimation approach that closely follows methodological recommendations in the technical appendices of the Uniform Methods Project for Whole Home Building Analysis and the California Evaluation Project, with some modifications and more specific guidance developed through empirical testing to ensure consistency and replicability of results.\n\n\nThe two-stage approach first fits \ntwo\n separate parametric models to daily average energy use, on the pre-intervention (baseline) period and one in the post-intervention (reporting) period for a single site using an ordinary least squares regression of the form:\n\n\n\n\nUPD_{mi} = \\mu_i + \\beta_{Hi}H_m + \\beta_{Ci}C_m +  \\epsilon_{mi} \n\n\n\n\nWhere\n\n\n\n\nUPD_{mi}\n is average use (gas in therms, electricity in kWh) per day in month \nm\n for site \ni\n.\n\n\n\n\n\\mu_i\n is the mean use for site \ni\n, or intercept.\n\n\n\n\n\\beta_{Hi}\n is the coefficient site \ni\n on average heating degree days per day.\n\n\n\n\n\\beta_{Ci}\n is the coefficient or site \ni\n on average cooling degree days per day.\n\n\n\n\nH_m\n is the average number of heating degree days per day in month \nm\n, which is a function of a fixed base temperature, the average daily temperatures from the weather station matched to site \ni\n during month \nm\n, and the number of days in month \nm\n with matched usage and weather data for site \ni\n.\n\n\n\n\nC_m\n is the average number of cooling degree days per day in month \nm\n, which is a function of a selected base temperature, the average daily temperatures from the weather station matched to site \ni\n during month \nm\n, and the number of days in month \nm\n with matched usage and weather data for site \ni\n.\n\n\n\n\n\\epsilon_{mi}\n is the site specific error term for a given month.\n\n\nIn the second stage, using parameter estimates from the first stage equation, weather normalized savings for both the baseline period and reporting period can be computed by using corresponding temperature normals for the relevant time period (typical year weather normalized gross savings), or by using current-year weather to project forward baseline period use (current year weather normalized gross savings) and differencing between baseline and reporting period estimated or actual use, depending on the quantity of interest.\n\n\nThis site-level two-stage approach without the use of a comparison group, while having significant limitations and tradeoffs, was decided by the technical working group to be appropriate for the two main use cases for CalTRACK, which emphasize effects on the grid and feedback to software vendors, rather than causal programatic effects. In addition to its long history of use in the EM&V literature, it draws on a methodological foundation developed in the more general literature on piecewise linear regression or segmented regression for policy analysis and effect estimates that is used in fields as divers as public health, medical research, and econometrics.\n\n\nWe now proceed with a detailed technical treatment of the steps for monthly savings estimation.\n\n\nTechnical guidelines for implementing two-stage estimation on monthly electric and gas usage data for CalTRACK\n\n\nCalTRACK savings estimation begins with gas and electric usage data, project data, and weather data that have been cleaned and combined according to the Data Cleaning and Integration technical specification. Starting with the prepared data, site-level monthly gross savings analysis is performed by implementing the following steps:\n\n\n1. Generate Use Per Day values and separate usage data into a pre- and a post-intervention data series\n\n\nThe CalTRACK monthly gross savings analysis uses average use per day (\nUPD\n) values for each month by taking the bill-period usage values, then dividing by the number of days in that bill period, as follows:\n\n\n\n\nUPD_m = \\frac{1}{n_{U_d}} * \\sum{U_d}\n\n\n\n\nWhere\n\n\n\n\nUPD_m\n is the average use per day for a given month \nm\n\n\n\n\n\\sum{U_d}\n is the sum of all daily use values \nU_d\n for a given month `m\u2019\n\n\n\n\nn_{U_d}\n is the total number of daily use values provided in the usage series that are between the first calendar day of month \nm\n and the last calendar day of month \nm\n\n\nNote: If daily use data for gas or electric is not available, monthly billing data can be used for the monthly billing analysis. However, modifications of the denominators for average use per day and for average HDD and CDD per day are necessary.\n\n\nNow split the series of \nUPD_m\n values into pre- and post-intervention periods according to the following rules:\n\n\nPre-intervention period\n: all full month (15 days of recorded use or more) $UPD_m$ values from the beginning of the series up to the the complete calendar month prior to the \nwork_start_date\n. The month containing \nwork_start_date\n is excluded from this series.\n\n\nPost-intervention period\n: all full month (15 days of recorded use or more) values from the first complete calendar month after the \nwork_end_date\n to the end of the series.\n\n\nFinal data sufficiency qualification check\n: All qualifying sites must have at least 12 months of contiguous $UPD_m$ values in the pre-intervention series and at least 12 months of contiguous post-intervention \nUPD_m\n values starting with the month after \nwork_end_date\n.\n\n\nAll sites not meeting these minimum data requirements are thrown out of the analysis\n\n\n2. Set fixed degree day base temperature and calculated HDD and CDD\n\n\nNext you calculate total HDD and CDD for the each calendar month in the series. CalTRACK will use a fixed degree day base for monthly billing analysis. The following balance point temperatures will be use:\n\n\nHDD base temp: 60 F\n\n\nCDD base temp: 70 F\n\n\nHDD and CDD values are calculated as follows\n\n\n\n\nHDD_m = \\frac{1}{n_{U_d}} * \\sum{ \\max(60 - T_{ave}, 0) }\n\n\n\n\nWhere\n\n\n\n\nHDD_m\n = Average heating degree days per day for calendar month \nm\n\n\n\n\nn_{U_d}\n = the number of days with both weather and usage data\n\n\n\n\n\\sum\n = the sum of the degree  over each day \nd\n in calendar month \nm\n\n\n\n\n\\max\n = the maximum of the two values in ()\n\n\n\n\nT_{ave}\n = the average temperature for day \nd\n\n\nAnd\n\n\n\n\nCDD_m = \\frac{1}{n_{U_d}} * \\sum{ max(ave_temp_d - 70, 0) }\n\n\n\n\nWhere\n\n\n\n\nCDD_m\n = Cooling degree days for calendar month \nm\n\n\n\n\nn_{U_d}\n = the number of days with both weather and usage data\n\n\n\n\n\\sum\n = the sum of values in {} over each day \nd\n in calendar month \nm\n\n\n\n\n\\max\n = the maximum of the two values in ()\n\n\n\n\nT_{ave}\n = the average temperature for day \nd\n\n\nDaily average temperatures are taken from the GSOD average data temperature dataset provided by NOAA\n\n\n3. Fit All Candidate Models and Apply Qualification Criteria\n\n\nFor each site, all allowable models will be run as candidate models and then have minimum fitness criteria set for qualification.\n\n\nFor CalTRACK electric monthly savings analysis, the following candidate models are fit:\n\n\n\n\n UPD_{mi} = \\mu_i + \\beta_{Hi}H_m + \\beta_{Ci}C_m +  \\epsilon_{mi} \n\n\n\n\n\n\n UPD_{mi} = \\mu_i + \\beta_{Hi}H_m +  \\epsilon_{mi} \n\n\n\n\n\n\n UPD_{mi} = \\mu_i + \\beta_{Ci}C_m +  \\epsilon_{mi} \n\n\n\n\n\n\n UPD_{mi} = \\mu_i + \\epsilon_{mi} \n\n\n\n\nwith the constraints\n\n\n\n\n\\beta_{H} > 0\n\n\n\n\n\n\n\\beta_{C} > 0\n\n\n\n\n\n\n\\mu_i > 0\n\n\n\n\nFor electric, qualifying models for selection must have each parameter estimate meet the minimum significance criteria of $p < 0.1$ and are strictly positive. All qualifying models are considered for final model selection.\n\n\nFor CalTRACK gas monthly savings analysis, the following candidate models are fit:\n\n\n\n\nUPD_{mi} = \\mu_i + \\beta_{Hi}H_m +  \\epsilon_{mi} \n\n\n\n\n\n\nUPD_{mi} = \\mu_i + \\beta_{Ci}C_m +  \\epsilon_{mi} \n\n\n\n\n\n\nUPD_{mi} = \\mu_i + \\epsilon_{mi} \n\n\n\n\nwith the constraints\n\n\n\n\n\\beta_{H} > 0\n\n\n\n\n\n\n\\mu_i > 0\n\n\n\n\nIf each parameter estimate meets minimum significance criteria (p < 0.1) and are strictly positive, then the model is a qualifying model for inclusion in model selection.\n\n\n4. Select the best for pre-intervenion and post-intervention periods for use in second-stage savings estimation\n\n\nAll qualifying pre-intervention models are compared to each other and among qualifying models, the model with the maximum adjusted R-squared will be selected for second-stage savings estimation.\n\n\nFor the monthly billing analysis, because we are using fixed degree days instead of variable degree days, adjusted R-squared will be defined as\n\n\n\n\n adjR^2 = 1 - \\frac{SS_{res}/df_e}{SS_{tot}/df_t} \n\n\n\n\nWhere\n\n\n\n\n SS_{res} \n is the sum of squares of residuals\n\n\n\n\n df_e \n is the degrees of freedom of the estimate of the underlying population error variance, and is calculated using \nn-p-1\n, where \nn\n is the number of observations in the sample used to estimate the model and \np\n is the number of explanatory variables, not including the constant term and not including degree day base temperature as a parameter because it\u2019s fixed\n\n\n\n\n SS_{tot} \n is the total sum of squares\n\n\n\n\n df_t \n is the degrees of freedom of the estimate of the population variance of the dependent variable, and is calculated as \nn-1\n, were \nn\n is the size of the sample use to estimate the model\n\n\nAll qualifying post-intervention models are compared to each other and among qualifying models, the model with the maximum adjusted R-squared will be selected for second-stage savings estimation.\n\n\n5. Estimate second-stage gross savings quantities based on selected first stage pre- and post-intervention models\n\n\nDuring the second stage, up to five savings quantities will be estimated for each site that meets the minimum data sufficiency criteria for that savings statistic.\n\n\nCumulative gross savings over entire performance period\nYear one annualized actual gross savings in the the reporting (post-intervention) period\nYear two annualized actual gross savings in the the reporting (post-intervention) period\nYear one annualized gross savings in the normal year\nYear two annualized gross savings in the normal year\n\n\nThese site-level second stage quantities are calculated as follows:\n\n\nCumulative gross savings over entire performance period (site-level)\n\n\n\n\nCompute \npredicted_baseline_use\n for each complete calendar month after \nwork_end_date\n using parameter estimates from the stage one model from the pre-intervention (baseline) period model and the associated average degree days for each month in the post-intervention (reporting) period, ensuring that the same degree day values calculated for stage one model fits are use in stage two estimation.\n\n\nCompute \nmonthly_gross_savings\n = \npredicted_baseline_monthly_use - actual_monthly_use\n for every complete calendar months after \nwork_end_date\n for project\n\n\nSum  \nmonthly_gross_savings\n over every complete calendar month since \nwork_end_date\n.\n\n\n\n\nYear one gross savings from 1 to 12 months after site visit. (site-level)\n\n\n\n\nCompute \npredicted_baseline_use\n for each complete calendar month after \nwork_end_date\n until 12 calendar months after \nwork_end_date\n  using parameter estimates from the stage one model from the pre-intervention (baseline) period model and the associated average degree days for each month in the post-intervention (reporting) period, ensuring that the same degree day values calculated for stage one model fits are use in stage two estimation.\n\n\nCompute \nmonthly_gross_savings\n = \npredicted_baseline_monthly_use - actual_monthly_use\n for 12 complete calendar months after \nwork_end_date\n for project\n\n\nSum  \nmonthly_gross_savings\n over the 12 calendar months since \nwork_end_date\n.\n\n\n\n\nYear two gross savings from 13 to 24 months after site visit. (site-level)\n\n\n\n\nCompute \npredicted_baseline_use\n for each complete calendar month starting 13 months after \nwork_end_date\n until 24 calendar months after \nwork_end_date\n  using parameter estimates from the stage one model from the pre-intervention (baseline) period model and the associated average degree days for each month in the post-intervention (reporting) period, ensuring that the same degree day values calculated for stage one model fits are use in stage two estimation.\n\n\nCompute \nmonthly_gross_savings\n = \npredicted_baseline_monthly_use - actual_monthly_use\n for month 13 to month 24 after \nwork_end_date\n for project\n\n\nSum  \nmonthly_gross_savings\n over the 12 calendar months from 13 months after \nwork_end_date\n to 24 months.\n\n\n\n\nYear one site-level annualized gross savings in the normal year\n\n\n\n\nCompute \npredicted_baseline_monthly_use\n using the stage one model from the baseline period and average degree days from the CZ2010 normal weather year. Use the full month of available values when calculating the average degree days per calendar month for the normal year.\n\n\nCompute \npredicted_reporting_monthly_use\n using a \nstage one\n model fit to only the first 12 months of post-intervention values and degree days from the CZ2010 normal weather year file. Use the full month of available values when calculating the average degree days per calendar month for the normal year.\n\n\nCompute \nmonthly_normal_year_gross_savings\n = \npredicted_baseline_monthly_use - predicted_reporting_monthly_use\n for normal year months\n\n\nSum  \nmonthly_normal_year_gross_savings\n over entire normal year.\n\n\n\n\nYear two site-level annualized gross savings in the normal year\n\n\n\n\nCompute \npredicted_baseline_monthly_use\n using the stage one model from the baseline period and degree days from the CZ2010 normal weather year.\n\n\nCompute \npredicted_reporting_monthly_use\n using a \nstage one\n model fit to only the 13th-24th months of post-intervention values and degree days from the CZ2010 normal weather year file for the relevant months.\n\n\nCompute \nmonthly_normal_year_gross_savings\n = \npredicted_baseline_monthly_use - predicted_reporting_monthly_use\n for each normal year month.\n\n\nSum  \nmonthly_normal_year_gross_savings\n over entire normal year.\n\n\n\n\nPost-estimation steps and portfolio aggregation\n\n\nThe goal of CalTRACK is to develop replicable, consistent, and methodologically defensible estimators of savings over \nportfolios of homes\n. In order to do that, the above site-level savings quantities must be aggregated to get portfolio-level totals, means, and variances. Taking the site-level estimates, CalTRACK then performs a set of aggregation steps that are specified \nhere\n.\n\n\n\n\nMonthly Savings Estimation Summary Statistics for Analysis Comparison\n\n\nTo ensure that the CalTRACK analysis specification can produce consistent results, each beta tester will generate a set of summary statistics on each of the above site-level savings estimates that can be shared with the larger group through csvs saved to this repository. There will be one savings summary file generated by each Beta Tester. Each file will be a .csv and will have the following general format:\n\n\n\n\n\n\n\n\nSummary Stat\n\n\nValue\n\n\n\n\n\n\n\n\n\n\nNumber of Sites Included in Analysis\n\n\n4321\n\n\n\n\n\n\nNumber of sites that met model fitness criteria\n\n\n3800\n\n\n\n\n\n\nMin cumulative gross savings\n\n\n123\n\n\n\n\n\n\nMax cumulative gross savings\n\n\n456\n\n\n\n\n\n\n\n\nCombined Project Data Summary File\n\nOutput Filename: \nmonthly_billing_analysis_savings_summary_NAME_OF_TESTER.csv\n\n\nIncluded Summary statistics\n\n\n\n\nTotal number of sites included in first-stage estimation\n\n\nTotal number of sites with cumulative gross savings estimates\n\n\nTotal number of sites with normal year annualized gross savings\n\n\nTotal number of sites with year one annualized gross savings\n\n\nTotal number of sites with year two annualized gross savings\n\n\nMin cumulative gross savings\n\n\nMax cumulative gross savings\n\n\nAverage cumulative gross savings\n\n\n10th percentile value cumulative gross savings\n\n\n20th percentile value cumulative gross savings\n\n\n30th percentile value cumulative gross savings\n\n\n40th percentile value cumulative gross savings\n\n\n50th percentile value cumulative gross savings\n\n\n60th percentile value cumulative gross savings\n\n\n70th percentile value cumulative gross savings\n\n\n80th percentile value cumulative gross savings\n\n\n90th percentile value cumulative gross savings\n\n\nCumulative gross savings average MSE\n\n\nCumulative gross savings average prediction error\n\n\nMin normal year annualized savings\n\n\nMax normal year annualized savings\n\n\nAverage normal year annualized savings\n\n\n10th percentile value normal year annualized savings\n\n\n20th percentile value normal year annualized savings\n\n\n30th percentile value normal year annualized savings\n\n\n40th percentile value normal year annualized savings\n\n\n50th percentile value normal year annualized savings\n\n\n60th percentile value normal year annualized savings\n\n\n70th percentile value normal year annualized savings\n\n\n80th percentile value normal year annualized savings\n\n\n90th percentile value normal year annualized savings\n\n\nNormal year annualized savings average MSE\n\n\nNormal year annualized savings average prediction error\n\n\nMin year one annualized gross savings\n\n\nMax year one annualized gross savings\n\n\nAverage year one annualized gross savings\n\n\n10th percentile value year one annualized gross savings\n\n\n20th percentile value year one annualized gross savings\n\n\n30th percentile value year one annualized gross savings\n\n\n40th percentile value year one annualized gross savings\n\n\n50th percentile value year one annualized gross savings\n\n\n60th percentile value year one annualized gross savings\n\n\n70th percentile value year one annualized gross savings\n\n\n80th percentile value year one annualized gross savings\n\n\n90th percentile value year one annualized gross savings\n\n\nYear one annualized gross savings average MSE\n\n\nYear one annualized gross savings average prediction error\n\n\nMin year two annualized gross savings\n\n\nMax year two annualized gross savings\n\n\nAverage year two annualized gross savings\n\n\n10th percentile value year two annualized gross savings\n\n\n20th percentile value year two annualized gross savings\n\n\n30th percentile value year two annualized gross savings\n\n\n40th percentile value year two annualized gross savings\n\n\n50th percentile value year two annualized gross savings\n\n\n60th percentile value year two annualized gross savings\n\n\n70th percentile value year two annualized gross savings\n\n\n80th percentile value year two annualized gross savings\n\n\n90th percentile value year two annualized gross savings\n\n\nYear two annualized gross savings average MSE\n\n\nYear two annualized gross savings average prediction error\n\n\nMin heating balance point temp baseline period\n\n\nMax heating balance point temp baseline period\n\n\nAverage heating balance point temp baseline period\n\n\n10th percentile value heating balance point temp baseline period\n\n\n20th percentile value heating balance point temp baseline period\n\n\n30th percentile value heating balance point temp baseline period\n\n\n40th percentile value heating balance point temp baseline period\n\n\n50th percentile value heating balance point temp baseline period\n\n\n60th percentile value heating balance point temp baseline period\n\n\n70th percentile value heating balance point temp baseline period\n\n\n80th percentile value heating balance point temp baseline period\n\n\n90th percentile value heating balance point temp baseline period\n\n\nMin cooling balance point temp baseline period\n\n\nMax cooling balance point temp baseline period\n\n\nAverage cooling balance point temp baseline period\n\n\n10th percentile value cooling balance point temp baseline period\n\n\n20th percentile value cooling balance point temp baseline period\n\n\n30th percentile value cooling balance point temp baseline period\n\n\n40th percentile value cooling balance point temp baseline period\n\n\n50th percentile value cooling balance point temp baseline period\n\n\n60th percentile value cooling balance point temp baseline period\n\n\n70th percentile value cooling balance point temp baseline period\n\n\n80th percentile value cooling balance point temp baseline period\n\n\n90th percentile value cooling balance point temp baseline period\n\n\nCount of Heating + Cooling models baseline period\n\n\nCount of Heating only models baseline period\n\n\nCount of Cooling only models baseline period\n\n\nMean Heating coefficient value across all baseline period models\n\n\nMin Heating coefficient value across all baseline period models\n\n\nMax Heating coefficient value across all baseline period models\n\n\nMean Cooling coefficient value across all baseline period models\n\n\nMin Cooling coefficient value across all baseline period models\n\n\nMax Cooling coefficient value across all baseline period models\n\n\nCount of sites where model selection changes between baseline and reporting (from \nN\n parameters to \nM\n parameters where \nN != M\n)\n\n\nMin cooling balance point temp reporting period\n\n\nMax cooling balance point temp reporting period\n\n\nAverage cooling balance point temp reporting period\n\n\nMin heating balance point temp reporting period\n\n\nMax heating balance point temp reporting period\n\n\nAverage heating balance point temp reporting period",
            "title": "Analysis of Monthly Data"
        },
        {
            "location": "/monthly/analysis/#caltrack-site-level-monthly-gross-savings-estimation-technical-guideline",
            "text": "",
            "title": "CalTRACK Site-level Monthly Gross Savings Estimation Technical Guideline"
        },
        {
            "location": "/monthly/analysis/#methodological-overview",
            "text": "Site-level gross savings using monthly billing data (both electricity and gas) will use a two-stage estimation approach that closely follows methodological recommendations in the technical appendices of the Uniform Methods Project for Whole Home Building Analysis and the California Evaluation Project, with some modifications and more specific guidance developed through empirical testing to ensure consistency and replicability of results.  The two-stage approach first fits  two  separate parametric models to daily average energy use, on the pre-intervention (baseline) period and one in the post-intervention (reporting) period for a single site using an ordinary least squares regression of the form:   UPD_{mi} = \\mu_i + \\beta_{Hi}H_m + \\beta_{Ci}C_m +  \\epsilon_{mi}    Where   UPD_{mi}  is average use (gas in therms, electricity in kWh) per day in month  m  for site  i .   \\mu_i  is the mean use for site  i , or intercept.   \\beta_{Hi}  is the coefficient site  i  on average heating degree days per day.   \\beta_{Ci}  is the coefficient or site  i  on average cooling degree days per day.   H_m  is the average number of heating degree days per day in month  m , which is a function of a fixed base temperature, the average daily temperatures from the weather station matched to site  i  during month  m , and the number of days in month  m  with matched usage and weather data for site  i .   C_m  is the average number of cooling degree days per day in month  m , which is a function of a selected base temperature, the average daily temperatures from the weather station matched to site  i  during month  m , and the number of days in month  m  with matched usage and weather data for site  i .   \\epsilon_{mi}  is the site specific error term for a given month.  In the second stage, using parameter estimates from the first stage equation, weather normalized savings for both the baseline period and reporting period can be computed by using corresponding temperature normals for the relevant time period (typical year weather normalized gross savings), or by using current-year weather to project forward baseline period use (current year weather normalized gross savings) and differencing between baseline and reporting period estimated or actual use, depending on the quantity of interest.  This site-level two-stage approach without the use of a comparison group, while having significant limitations and tradeoffs, was decided by the technical working group to be appropriate for the two main use cases for CalTRACK, which emphasize effects on the grid and feedback to software vendors, rather than causal programatic effects. In addition to its long history of use in the EM&V literature, it draws on a methodological foundation developed in the more general literature on piecewise linear regression or segmented regression for policy analysis and effect estimates that is used in fields as divers as public health, medical research, and econometrics.  We now proceed with a detailed technical treatment of the steps for monthly savings estimation.",
            "title": "Methodological Overview"
        },
        {
            "location": "/monthly/analysis/#technical-guidelines-for-implementing-two-stage-estimation-on-monthly-electric-and-gas-usage-data-for-caltrack",
            "text": "CalTRACK savings estimation begins with gas and electric usage data, project data, and weather data that have been cleaned and combined according to the Data Cleaning and Integration technical specification. Starting with the prepared data, site-level monthly gross savings analysis is performed by implementing the following steps:",
            "title": "Technical guidelines for implementing two-stage estimation on monthly electric and gas usage data for CalTRACK"
        },
        {
            "location": "/monthly/analysis/#1-generate-use-per-day-values-and-separate-usage-data-into-a-pre-and-a-post-intervention-data-series",
            "text": "The CalTRACK monthly gross savings analysis uses average use per day ( UPD ) values for each month by taking the bill-period usage values, then dividing by the number of days in that bill period, as follows:   UPD_m = \\frac{1}{n_{U_d}} * \\sum{U_d}   Where   UPD_m  is the average use per day for a given month  m   \\sum{U_d}  is the sum of all daily use values  U_d  for a given month `m\u2019   n_{U_d}  is the total number of daily use values provided in the usage series that are between the first calendar day of month  m  and the last calendar day of month  m  Note: If daily use data for gas or electric is not available, monthly billing data can be used for the monthly billing analysis. However, modifications of the denominators for average use per day and for average HDD and CDD per day are necessary.  Now split the series of  UPD_m  values into pre- and post-intervention periods according to the following rules:  Pre-intervention period : all full month (15 days of recorded use or more) $UPD_m$ values from the beginning of the series up to the the complete calendar month prior to the  work_start_date . The month containing  work_start_date  is excluded from this series.  Post-intervention period : all full month (15 days of recorded use or more) values from the first complete calendar month after the  work_end_date  to the end of the series.  Final data sufficiency qualification check : All qualifying sites must have at least 12 months of contiguous $UPD_m$ values in the pre-intervention series and at least 12 months of contiguous post-intervention  UPD_m  values starting with the month after  work_end_date .  All sites not meeting these minimum data requirements are thrown out of the analysis",
            "title": "1. Generate Use Per Day values and separate usage data into a pre- and a post-intervention data series"
        },
        {
            "location": "/monthly/analysis/#2-set-fixed-degree-day-base-temperature-and-calculated-hdd-and-cdd",
            "text": "Next you calculate total HDD and CDD for the each calendar month in the series. CalTRACK will use a fixed degree day base for monthly billing analysis. The following balance point temperatures will be use:  HDD base temp: 60 F  CDD base temp: 70 F  HDD and CDD values are calculated as follows   HDD_m = \\frac{1}{n_{U_d}} * \\sum{ \\max(60 - T_{ave}, 0) }   Where   HDD_m  = Average heating degree days per day for calendar month  m   n_{U_d}  = the number of days with both weather and usage data   \\sum  = the sum of the degree  over each day  d  in calendar month  m   \\max  = the maximum of the two values in ()   T_{ave}  = the average temperature for day  d  And   CDD_m = \\frac{1}{n_{U_d}} * \\sum{ max(ave_temp_d - 70, 0) }   Where   CDD_m  = Cooling degree days for calendar month  m   n_{U_d}  = the number of days with both weather and usage data   \\sum  = the sum of values in {} over each day  d  in calendar month  m   \\max  = the maximum of the two values in ()   T_{ave}  = the average temperature for day  d  Daily average temperatures are taken from the GSOD average data temperature dataset provided by NOAA",
            "title": "2. Set fixed degree day base temperature and calculated HDD and CDD"
        },
        {
            "location": "/monthly/analysis/#3-fit-all-candidate-models-and-apply-qualification-criteria",
            "text": "For each site, all allowable models will be run as candidate models and then have minimum fitness criteria set for qualification.  For CalTRACK electric monthly savings analysis, the following candidate models are fit:    UPD_{mi} = \\mu_i + \\beta_{Hi}H_m + \\beta_{Ci}C_m +  \\epsilon_{mi}      UPD_{mi} = \\mu_i + \\beta_{Hi}H_m +  \\epsilon_{mi}      UPD_{mi} = \\mu_i + \\beta_{Ci}C_m +  \\epsilon_{mi}      UPD_{mi} = \\mu_i + \\epsilon_{mi}    with the constraints   \\beta_{H} > 0    \\beta_{C} > 0    \\mu_i > 0   For electric, qualifying models for selection must have each parameter estimate meet the minimum significance criteria of $p < 0.1$ and are strictly positive. All qualifying models are considered for final model selection.  For CalTRACK gas monthly savings analysis, the following candidate models are fit:   UPD_{mi} = \\mu_i + \\beta_{Hi}H_m +  \\epsilon_{mi}     UPD_{mi} = \\mu_i + \\beta_{Ci}C_m +  \\epsilon_{mi}     UPD_{mi} = \\mu_i + \\epsilon_{mi}    with the constraints   \\beta_{H} > 0    \\mu_i > 0   If each parameter estimate meets minimum significance criteria (p < 0.1) and are strictly positive, then the model is a qualifying model for inclusion in model selection.",
            "title": "3. Fit All Candidate Models and Apply Qualification Criteria"
        },
        {
            "location": "/monthly/analysis/#4-select-the-best-for-pre-intervenion-and-post-intervention-periods-for-use-in-second-stage-savings-estimation",
            "text": "All qualifying pre-intervention models are compared to each other and among qualifying models, the model with the maximum adjusted R-squared will be selected for second-stage savings estimation.  For the monthly billing analysis, because we are using fixed degree days instead of variable degree days, adjusted R-squared will be defined as    adjR^2 = 1 - \\frac{SS_{res}/df_e}{SS_{tot}/df_t}    Where    SS_{res}   is the sum of squares of residuals    df_e   is the degrees of freedom of the estimate of the underlying population error variance, and is calculated using  n-p-1 , where  n  is the number of observations in the sample used to estimate the model and  p  is the number of explanatory variables, not including the constant term and not including degree day base temperature as a parameter because it\u2019s fixed    SS_{tot}   is the total sum of squares    df_t   is the degrees of freedom of the estimate of the population variance of the dependent variable, and is calculated as  n-1 , were  n  is the size of the sample use to estimate the model  All qualifying post-intervention models are compared to each other and among qualifying models, the model with the maximum adjusted R-squared will be selected for second-stage savings estimation.",
            "title": "4. Select the best for pre-intervenion and post-intervention periods for use in second-stage savings estimation"
        },
        {
            "location": "/monthly/analysis/#5-estimate-second-stage-gross-savings-quantities-based-on-selected-first-stage-pre-and-post-intervention-models",
            "text": "During the second stage, up to five savings quantities will be estimated for each site that meets the minimum data sufficiency criteria for that savings statistic.  Cumulative gross savings over entire performance period\nYear one annualized actual gross savings in the the reporting (post-intervention) period\nYear two annualized actual gross savings in the the reporting (post-intervention) period\nYear one annualized gross savings in the normal year\nYear two annualized gross savings in the normal year  These site-level second stage quantities are calculated as follows:",
            "title": "5. Estimate second-stage gross savings quantities based on selected first stage pre- and post-intervention models"
        },
        {
            "location": "/monthly/analysis/#cumulative-gross-savings-over-entire-performance-period-site-level",
            "text": "Compute  predicted_baseline_use  for each complete calendar month after  work_end_date  using parameter estimates from the stage one model from the pre-intervention (baseline) period model and the associated average degree days for each month in the post-intervention (reporting) period, ensuring that the same degree day values calculated for stage one model fits are use in stage two estimation.  Compute  monthly_gross_savings  =  predicted_baseline_monthly_use - actual_monthly_use  for every complete calendar months after  work_end_date  for project  Sum   monthly_gross_savings  over every complete calendar month since  work_end_date .",
            "title": "Cumulative gross savings over entire performance period (site-level)"
        },
        {
            "location": "/monthly/analysis/#year-one-gross-savings-from-1-to-12-months-after-site-visit-site-level",
            "text": "Compute  predicted_baseline_use  for each complete calendar month after  work_end_date  until 12 calendar months after  work_end_date   using parameter estimates from the stage one model from the pre-intervention (baseline) period model and the associated average degree days for each month in the post-intervention (reporting) period, ensuring that the same degree day values calculated for stage one model fits are use in stage two estimation.  Compute  monthly_gross_savings  =  predicted_baseline_monthly_use - actual_monthly_use  for 12 complete calendar months after  work_end_date  for project  Sum   monthly_gross_savings  over the 12 calendar months since  work_end_date .",
            "title": "Year one gross savings from 1 to 12 months after site visit. (site-level)"
        },
        {
            "location": "/monthly/analysis/#year-two-gross-savings-from-13-to-24-months-after-site-visit-site-level",
            "text": "Compute  predicted_baseline_use  for each complete calendar month starting 13 months after  work_end_date  until 24 calendar months after  work_end_date   using parameter estimates from the stage one model from the pre-intervention (baseline) period model and the associated average degree days for each month in the post-intervention (reporting) period, ensuring that the same degree day values calculated for stage one model fits are use in stage two estimation.  Compute  monthly_gross_savings  =  predicted_baseline_monthly_use - actual_monthly_use  for month 13 to month 24 after  work_end_date  for project  Sum   monthly_gross_savings  over the 12 calendar months from 13 months after  work_end_date  to 24 months.",
            "title": "Year two gross savings from 13 to 24 months after site visit. (site-level)"
        },
        {
            "location": "/monthly/analysis/#year-one-site-level-annualized-gross-savings-in-the-normal-year",
            "text": "Compute  predicted_baseline_monthly_use  using the stage one model from the baseline period and average degree days from the CZ2010 normal weather year. Use the full month of available values when calculating the average degree days per calendar month for the normal year.  Compute  predicted_reporting_monthly_use  using a  stage one  model fit to only the first 12 months of post-intervention values and degree days from the CZ2010 normal weather year file. Use the full month of available values when calculating the average degree days per calendar month for the normal year.  Compute  monthly_normal_year_gross_savings  =  predicted_baseline_monthly_use - predicted_reporting_monthly_use  for normal year months  Sum   monthly_normal_year_gross_savings  over entire normal year.",
            "title": "Year one site-level annualized gross savings in the normal year"
        },
        {
            "location": "/monthly/analysis/#year-two-site-level-annualized-gross-savings-in-the-normal-year",
            "text": "Compute  predicted_baseline_monthly_use  using the stage one model from the baseline period and degree days from the CZ2010 normal weather year.  Compute  predicted_reporting_monthly_use  using a  stage one  model fit to only the 13th-24th months of post-intervention values and degree days from the CZ2010 normal weather year file for the relevant months.  Compute  monthly_normal_year_gross_savings  =  predicted_baseline_monthly_use - predicted_reporting_monthly_use  for each normal year month.  Sum   monthly_normal_year_gross_savings  over entire normal year.",
            "title": "Year two site-level annualized gross savings in the normal year"
        },
        {
            "location": "/monthly/analysis/#post-estimation-steps-and-portfolio-aggregation",
            "text": "The goal of CalTRACK is to develop replicable, consistent, and methodologically defensible estimators of savings over  portfolios of homes . In order to do that, the above site-level savings quantities must be aggregated to get portfolio-level totals, means, and variances. Taking the site-level estimates, CalTRACK then performs a set of aggregation steps that are specified  here .",
            "title": "Post-estimation steps and portfolio aggregation"
        },
        {
            "location": "/monthly/analysis/#monthly-savings-estimation-summary-statistics-for-analysis-comparison",
            "text": "To ensure that the CalTRACK analysis specification can produce consistent results, each beta tester will generate a set of summary statistics on each of the above site-level savings estimates that can be shared with the larger group through csvs saved to this repository. There will be one savings summary file generated by each Beta Tester. Each file will be a .csv and will have the following general format:     Summary Stat  Value      Number of Sites Included in Analysis  4321    Number of sites that met model fitness criteria  3800    Min cumulative gross savings  123    Max cumulative gross savings  456     Combined Project Data Summary File \nOutput Filename:  monthly_billing_analysis_savings_summary_NAME_OF_TESTER.csv",
            "title": "Monthly Savings Estimation Summary Statistics for Analysis Comparison"
        },
        {
            "location": "/monthly/analysis/#included-summary-statistics",
            "text": "Total number of sites included in first-stage estimation  Total number of sites with cumulative gross savings estimates  Total number of sites with normal year annualized gross savings  Total number of sites with year one annualized gross savings  Total number of sites with year two annualized gross savings  Min cumulative gross savings  Max cumulative gross savings  Average cumulative gross savings  10th percentile value cumulative gross savings  20th percentile value cumulative gross savings  30th percentile value cumulative gross savings  40th percentile value cumulative gross savings  50th percentile value cumulative gross savings  60th percentile value cumulative gross savings  70th percentile value cumulative gross savings  80th percentile value cumulative gross savings  90th percentile value cumulative gross savings  Cumulative gross savings average MSE  Cumulative gross savings average prediction error  Min normal year annualized savings  Max normal year annualized savings  Average normal year annualized savings  10th percentile value normal year annualized savings  20th percentile value normal year annualized savings  30th percentile value normal year annualized savings  40th percentile value normal year annualized savings  50th percentile value normal year annualized savings  60th percentile value normal year annualized savings  70th percentile value normal year annualized savings  80th percentile value normal year annualized savings  90th percentile value normal year annualized savings  Normal year annualized savings average MSE  Normal year annualized savings average prediction error  Min year one annualized gross savings  Max year one annualized gross savings  Average year one annualized gross savings  10th percentile value year one annualized gross savings  20th percentile value year one annualized gross savings  30th percentile value year one annualized gross savings  40th percentile value year one annualized gross savings  50th percentile value year one annualized gross savings  60th percentile value year one annualized gross savings  70th percentile value year one annualized gross savings  80th percentile value year one annualized gross savings  90th percentile value year one annualized gross savings  Year one annualized gross savings average MSE  Year one annualized gross savings average prediction error  Min year two annualized gross savings  Max year two annualized gross savings  Average year two annualized gross savings  10th percentile value year two annualized gross savings  20th percentile value year two annualized gross savings  30th percentile value year two annualized gross savings  40th percentile value year two annualized gross savings  50th percentile value year two annualized gross savings  60th percentile value year two annualized gross savings  70th percentile value year two annualized gross savings  80th percentile value year two annualized gross savings  90th percentile value year two annualized gross savings  Year two annualized gross savings average MSE  Year two annualized gross savings average prediction error  Min heating balance point temp baseline period  Max heating balance point temp baseline period  Average heating balance point temp baseline period  10th percentile value heating balance point temp baseline period  20th percentile value heating balance point temp baseline period  30th percentile value heating balance point temp baseline period  40th percentile value heating balance point temp baseline period  50th percentile value heating balance point temp baseline period  60th percentile value heating balance point temp baseline period  70th percentile value heating balance point temp baseline period  80th percentile value heating balance point temp baseline period  90th percentile value heating balance point temp baseline period  Min cooling balance point temp baseline period  Max cooling balance point temp baseline period  Average cooling balance point temp baseline period  10th percentile value cooling balance point temp baseline period  20th percentile value cooling balance point temp baseline period  30th percentile value cooling balance point temp baseline period  40th percentile value cooling balance point temp baseline period  50th percentile value cooling balance point temp baseline period  60th percentile value cooling balance point temp baseline period  70th percentile value cooling balance point temp baseline period  80th percentile value cooling balance point temp baseline period  90th percentile value cooling balance point temp baseline period  Count of Heating + Cooling models baseline period  Count of Heating only models baseline period  Count of Cooling only models baseline period  Mean Heating coefficient value across all baseline period models  Min Heating coefficient value across all baseline period models  Max Heating coefficient value across all baseline period models  Mean Cooling coefficient value across all baseline period models  Min Cooling coefficient value across all baseline period models  Max Cooling coefficient value across all baseline period models  Count of sites where model selection changes between baseline and reporting (from  N  parameters to  M  parameters where  N != M )  Min cooling balance point temp reporting period  Max cooling balance point temp reporting period  Average cooling balance point temp reporting period  Min heating balance point temp reporting period  Max heating balance point temp reporting period  Average heating balance point temp reporting period",
            "title": "Included Summary statistics"
        },
        {
            "location": "/monthly/aggregation/",
            "text": "Technical Specification for Aggregating Site-level Gross Savings and Quantifying Uncertainty in Aggregate Savings Statistics\n\n\n\n\nThe goal of CalTRACK is to develop replicable, consistent, and methodologically defensible estimators of savings over \nportfolios of homes\n.\n\n\nPortfolio-level savings statistics are based on aggregations of site-level savings gross savings estimates created using the CalTRACK site-level monthly gross savings analysis methods.\n\n\nBecause all site-level savings quantities generated using CalTRACK's technical specifications are based on time series predictions, portfolio-level savings statistics primarily use prediction errors to estimate of site-level uncertainty, and calculate portfolio-level averages using inverse variance weighted means to get a consistent estimator of mean portfolio-level savings.\n\n\nMonthly Savings Estimate Aggregation Procedure\n\n\nThe main portfolio-level statistics of interest for CalTRACK are:\n\n\n\n\nWeighted mean annualized gross savings\n\n\nVariance annualized gross savings\n\n\nAnnualized gross savings prediction intervals (+/- 95%)\n\n\nUnweighted total annualized gross savings\n\n\nWeighted mean cumulative gross savings\n\n\nCumulative gross savings prediction intervals (+/-95%)\n\n\nUnweighted total cumulative gross savings\n\n\nWeighted mean year-one gross savings\n\n\nYear-one gross savings prediction intervals (+/-95%)\n\n\nUnweighted total year-one gross savings\n\n\nWeighted mean year-two gross savings\n\n\nYear-two gross savings prediction intervals (+/-95%)\n\n\nUnweighted total year-two gross savings\n\n\n\n\nSteps for calculating aggregate uncertainty and aggregate means.\n\n\nWhile a detailed treatment of how to calculate each of these quantities is included below, the main formulations are below:\n\n\n\n\nCalculate the site-level Mean Squared Error (MSE) as an unbias estimator of the variance of the model errors, $s^2$ :\n\n\n\n\n\n\ns^2 = \\sum{\\frac{\\hat{u}_i^2}{N\u2212k}}\n\n\n\n\n\n\nCalculate the site-level savings variance using in prediction error as an consistent estimator using the MSE, $s^2$, and variance in the out-of-sample data, $x_0$:\n\n\n\n\n\n\n\\hat{V}_s = s^2*x_0*(X'X)^{\u22121} * x_0' + s^2\n\n\n\n\n\n\nCalculate portfolio site-level inverse variance weighted mean savings using the savings variance and the following equation:\n\n\n\n\n\n\nPrepared Summary Statistics for Aggregation Comparison\n\n\nTo ensure that the CalTRACK analysis specification can produce consistent results, each beta tester will generate a set of aggregate statistics on each of the above site-level savings estimates that can be shared with the larger group through csvs saved to this repository.\n\n\nThere will be one savings summary file generated by each Beta Tester. Each file will be a .csv and will have the following general format:\n\n\n\n\n\n\n\n\nSummary Stat\n\n\nValue\n\n\n\n\n\n\n\n\n\n\nNumber of sites included in aggregation\n\n\n4321\n\n\n\n\n\n\nIVW mean annualized gross savings\n\n\n123\n\n\n\n\n\n\nPorfolio forecast variance\n\n\n4156\n\n\n\n\n\n\n\n\nCombined Project Data Summary File\n\n\nOutput Filename: \nmonthly_billing_analysis_savings_portfolio_aggregation_NAME_OF_TESTER.csv\n\n\nIncluded Summary statistics for Portfolios of Sites\n\n\n\n\nWeighted mean annualized gross savings\n\n\nVariance annualized gross savings\n\n\nAnnualized gross savings prediction intervals (+/- 95%)\n\n\nUnweighted total annualized gross savings\n\n\nWeighted mean cumulative gross savings\n\n\nCumulative gross savings prediction intervals (+/-95%)\n\n\nUnweighted total cumulative gross savings\n\n\nWeighted mean year-one gross savings\n\n\nYear-one gross savings prediction intervals (+/-95%)\n\n\nUnweighted total year-one gross savings\n\n\nWeighted mean year-two gross savings\n\n\nYear-two gross savings prediction intervals (+/-95%)\n\n\nUnweighted total year-two gross savings\n\n\n\n\nCalculating site-level prediction intervals\n\n\nFor simplicity, and keeping with convention in the industry, site-level variance estimates based on ordinary least squares regressions will use OLS prediction error as the estimator for savings variance. Prediction error is calculated as follows:\n\n\nTake the stage one regression model with N observations and k regressors:\n\n\n\n\ny = X\\beta + u\n\n\n\n\nGiven a vector (or matrix) $x_0$ of post-intervention (reporting) period degree day covariates, the predicted value for  observation would be\n\n\n\n\nE[y|x_0] = \\hat{y}_0 = x_0\\beta\n\n\n\n\nA consistent estimator of the variance of this prediction is\n\n\n\n\n\\hat{V}_p = s^2*x_0*(X'X)^{\u22121} * x_0\n\n\n\n\nwhere\n\n\n\n\ns^2 = \\sum{\\frac{\\hat{u}_i^2}{(N-k)}}\n\n\n\n\nand \nX\n is the matrix of stage one covariates.\n\n\nThe forecast error for a particular $y_0$ is\n\n\n\n\n\\hat{e} = y_0 \u2212 \\hat{y}_0= x_0\\beta + u_0 \u2212 \\hat{y}_0\n\n\n\n\nThe zero covariance between $u_0$ and $\\hat{\u03b2}$ implies that\n\n\n\n\nVar[\\hat{e}] = Var[\\hat{y}_0] + Var[u_0]\n\n\n\n\nand a consistent estimator of that is\n\n\n\n\n\\hat{V}_s=s^2*x_0*(X'X)^{\u22121} * x_0' + s^2\n\n\n\n\nThe \n1\u2212\\alpha\n site-level confidence interval will be:\n\n\n\n\ny_0 \u00b1 t_(1\u2212\\alpha/2) * (\\hat{V}_p)^.5\n\n\n\n\nThe \n1\u2212\u03b1\n confidence interval on the savings will be wider, based on the estimated savings variance:\n\n\n\n\ny_0 \u00b1 t_(1\u2212\\alpha/2) * (\\hat{V}_s)^.5\n\n\n\n\nCalculating Inverse-variance Weighted Portfolio Savings Means for Monthly Savings Analysis\n\n\nUsing site-level forecast variance as the consistent estimator of site-level gross savings estimation, CalTRACK will calculate the inverse-variance weighted mean for each portfolio according to the following equation:\n\n\n\n\n\\hat{y} = \\frac{\\sum_i{y_i/ \\sigma_i^2}}{\\sum_i{1/ \\sigma^2_i}}\n\n\n\n\nCalculating uncertainty for daily and hourly methods\n\n\nWhile sampling methods would actually be preferable for characterizing the posterior distribution of savings estimates using higher-frequency AMI data, due to lack of adoption of Bayesian methods in industry and increased computational complexity, we propose CalTRACK use more frequently adopted.\n\n\nThe two primary considerations for higher-frequency savings models are the need to take into account stronger autocorrelation and increased model specification error.\n\n\nM & V standards for industrial savings estimation, which has been dealing with AMI data longer, provides useful guidance for dealing with these two considerations. Following ASHRAE Guideline 14-2002, and augmenting work done by the NW SEM Collaborative, we propose the following method:\n\n\nFractional Savings uncertainty calculation\n\n\nBecause variances are larger in larger homes, normalizing levels of uncertainty using fractional savings uncertainty is an important aggregate metric, both for model comparison and selection, as well as final output. CalTRACK will compute the Fractional Savings Uncertainty at the site level based on the following equation:\n\n\n\n\nWhere\n\n\n$CV$ is the coefficient of variance on the savings mean using prediction errors specified above\n$t$ is the relevant t-statistic for the desired level of confidence\n$F$ is the relevant F-statistic given degrees of freedom for the selected model\n\n\nCalculating 95% confidence intervals using fractional savings uncertainty\n\n\nTo calculate confidence intervals using the following equation:\n\n\n\n\nCI(95) = +/- (FSU * 1.96) * 100\n\n\n\n\nNote on the lack of comparison group adjustments in CalTRACK technical specification\n\n\nWhile the technical working group acknowledged the potential use of comparison groups in gross savings estimation to correct for population-wide exogenous effects on use, after extensive debate, it was decided that the CalTRACK use cases (pay-for-performance in particular) required the ability for non-utility actors to be able to estimate savings without access to comparison group data. While several promising ideas were developed through the technical working group process about how to address this issue (utilities publishing public adjustment factors based on pre-tabilated comparison groups for example), there were serious concern about feasbility in time for early 2017 roll out of the P4P pilot. As a result, the issue of transparent and replicable comparison group adjustments as part of CalTRACK methods was left for the next version of CalTRACK technical specifications.",
            "title": "Aggregration of Site-level Savings"
        },
        {
            "location": "/monthly/aggregation/#technical-specification-for-aggregating-site-level-gross-savings-and-quantifying-uncertainty-in-aggregate-savings-statistics",
            "text": "The goal of CalTRACK is to develop replicable, consistent, and methodologically defensible estimators of savings over  portfolios of homes .  Portfolio-level savings statistics are based on aggregations of site-level savings gross savings estimates created using the CalTRACK site-level monthly gross savings analysis methods.  Because all site-level savings quantities generated using CalTRACK's technical specifications are based on time series predictions, portfolio-level savings statistics primarily use prediction errors to estimate of site-level uncertainty, and calculate portfolio-level averages using inverse variance weighted means to get a consistent estimator of mean portfolio-level savings.",
            "title": "Technical Specification for Aggregating Site-level Gross Savings and Quantifying Uncertainty in Aggregate Savings Statistics"
        },
        {
            "location": "/monthly/aggregation/#monthly-savings-estimate-aggregation-procedure",
            "text": "The main portfolio-level statistics of interest for CalTRACK are:   Weighted mean annualized gross savings  Variance annualized gross savings  Annualized gross savings prediction intervals (+/- 95%)  Unweighted total annualized gross savings  Weighted mean cumulative gross savings  Cumulative gross savings prediction intervals (+/-95%)  Unweighted total cumulative gross savings  Weighted mean year-one gross savings  Year-one gross savings prediction intervals (+/-95%)  Unweighted total year-one gross savings  Weighted mean year-two gross savings  Year-two gross savings prediction intervals (+/-95%)  Unweighted total year-two gross savings",
            "title": "Monthly Savings Estimate Aggregation Procedure"
        },
        {
            "location": "/monthly/aggregation/#steps-for-calculating-aggregate-uncertainty-and-aggregate-means",
            "text": "While a detailed treatment of how to calculate each of these quantities is included below, the main formulations are below:   Calculate the site-level Mean Squared Error (MSE) as an unbias estimator of the variance of the model errors, $s^2$ :    s^2 = \\sum{\\frac{\\hat{u}_i^2}{N\u2212k}}    Calculate the site-level savings variance using in prediction error as an consistent estimator using the MSE, $s^2$, and variance in the out-of-sample data, $x_0$:    \\hat{V}_s = s^2*x_0*(X'X)^{\u22121} * x_0' + s^2    Calculate portfolio site-level inverse variance weighted mean savings using the savings variance and the following equation:",
            "title": "Steps for calculating aggregate uncertainty and aggregate means."
        },
        {
            "location": "/monthly/aggregation/#prepared-summary-statistics-for-aggregation-comparison",
            "text": "To ensure that the CalTRACK analysis specification can produce consistent results, each beta tester will generate a set of aggregate statistics on each of the above site-level savings estimates that can be shared with the larger group through csvs saved to this repository.  There will be one savings summary file generated by each Beta Tester. Each file will be a .csv and will have the following general format:     Summary Stat  Value      Number of sites included in aggregation  4321    IVW mean annualized gross savings  123    Porfolio forecast variance  4156",
            "title": "Prepared Summary Statistics for Aggregation Comparison"
        },
        {
            "location": "/monthly/aggregation/#combined-project-data-summary-file",
            "text": "Output Filename:  monthly_billing_analysis_savings_portfolio_aggregation_NAME_OF_TESTER.csv",
            "title": "Combined Project Data Summary File"
        },
        {
            "location": "/monthly/aggregation/#included-summary-statistics-for-portfolios-of-sites",
            "text": "Weighted mean annualized gross savings  Variance annualized gross savings  Annualized gross savings prediction intervals (+/- 95%)  Unweighted total annualized gross savings  Weighted mean cumulative gross savings  Cumulative gross savings prediction intervals (+/-95%)  Unweighted total cumulative gross savings  Weighted mean year-one gross savings  Year-one gross savings prediction intervals (+/-95%)  Unweighted total year-one gross savings  Weighted mean year-two gross savings  Year-two gross savings prediction intervals (+/-95%)  Unweighted total year-two gross savings",
            "title": "Included Summary statistics for Portfolios of Sites"
        },
        {
            "location": "/monthly/aggregation/#calculating-site-level-prediction-intervals",
            "text": "For simplicity, and keeping with convention in the industry, site-level variance estimates based on ordinary least squares regressions will use OLS prediction error as the estimator for savings variance. Prediction error is calculated as follows:  Take the stage one regression model with N observations and k regressors:   y = X\\beta + u   Given a vector (or matrix) $x_0$ of post-intervention (reporting) period degree day covariates, the predicted value for  observation would be   E[y|x_0] = \\hat{y}_0 = x_0\\beta   A consistent estimator of the variance of this prediction is   \\hat{V}_p = s^2*x_0*(X'X)^{\u22121} * x_0   where   s^2 = \\sum{\\frac{\\hat{u}_i^2}{(N-k)}}   and  X  is the matrix of stage one covariates.  The forecast error for a particular $y_0$ is   \\hat{e} = y_0 \u2212 \\hat{y}_0= x_0\\beta + u_0 \u2212 \\hat{y}_0   The zero covariance between $u_0$ and $\\hat{\u03b2}$ implies that   Var[\\hat{e}] = Var[\\hat{y}_0] + Var[u_0]   and a consistent estimator of that is   \\hat{V}_s=s^2*x_0*(X'X)^{\u22121} * x_0' + s^2   The  1\u2212\\alpha  site-level confidence interval will be:   y_0 \u00b1 t_(1\u2212\\alpha/2) * (\\hat{V}_p)^.5   The  1\u2212\u03b1  confidence interval on the savings will be wider, based on the estimated savings variance:   y_0 \u00b1 t_(1\u2212\\alpha/2) * (\\hat{V}_s)^.5",
            "title": "Calculating site-level prediction intervals"
        },
        {
            "location": "/monthly/aggregation/#calculating-inverse-variance-weighted-portfolio-savings-means-for-monthly-savings-analysis",
            "text": "Using site-level forecast variance as the consistent estimator of site-level gross savings estimation, CalTRACK will calculate the inverse-variance weighted mean for each portfolio according to the following equation:   \\hat{y} = \\frac{\\sum_i{y_i/ \\sigma_i^2}}{\\sum_i{1/ \\sigma^2_i}}",
            "title": "Calculating Inverse-variance Weighted Portfolio Savings Means for Monthly Savings Analysis"
        },
        {
            "location": "/monthly/aggregation/#calculating-uncertainty-for-daily-and-hourly-methods",
            "text": "While sampling methods would actually be preferable for characterizing the posterior distribution of savings estimates using higher-frequency AMI data, due to lack of adoption of Bayesian methods in industry and increased computational complexity, we propose CalTRACK use more frequently adopted.  The two primary considerations for higher-frequency savings models are the need to take into account stronger autocorrelation and increased model specification error.  M & V standards for industrial savings estimation, which has been dealing with AMI data longer, provides useful guidance for dealing with these two considerations. Following ASHRAE Guideline 14-2002, and augmenting work done by the NW SEM Collaborative, we propose the following method:",
            "title": "Calculating uncertainty for daily and hourly methods"
        },
        {
            "location": "/monthly/aggregation/#fractional-savings-uncertainty-calculation",
            "text": "Because variances are larger in larger homes, normalizing levels of uncertainty using fractional savings uncertainty is an important aggregate metric, both for model comparison and selection, as well as final output. CalTRACK will compute the Fractional Savings Uncertainty at the site level based on the following equation:   Where  $CV$ is the coefficient of variance on the savings mean using prediction errors specified above\n$t$ is the relevant t-statistic for the desired level of confidence\n$F$ is the relevant F-statistic given degrees of freedom for the selected model",
            "title": "Fractional Savings uncertainty calculation"
        },
        {
            "location": "/monthly/aggregation/#calculating-95-confidence-intervals-using-fractional-savings-uncertainty",
            "text": "To calculate confidence intervals using the following equation:   CI(95) = +/- (FSU * 1.96) * 100",
            "title": "Calculating 95% confidence intervals using fractional savings uncertainty"
        },
        {
            "location": "/monthly/aggregation/#note-on-the-lack-of-comparison-group-adjustments-in-caltrack-technical-specification",
            "text": "While the technical working group acknowledged the potential use of comparison groups in gross savings estimation to correct for population-wide exogenous effects on use, after extensive debate, it was decided that the CalTRACK use cases (pay-for-performance in particular) required the ability for non-utility actors to be able to estimate savings without access to comparison group data. While several promising ideas were developed through the technical working group process about how to address this issue (utilities publishing public adjustment factors based on pre-tabilated comparison groups for example), there were serious concern about feasbility in time for early 2017 roll out of the P4P pilot. As a result, the issue of transparent and replicable comparison group adjustments as part of CalTRACK methods was left for the next version of CalTRACK technical specifications.",
            "title": "Note on the lack of comparison group adjustments in CalTRACK technical specification"
        }
    ]
}